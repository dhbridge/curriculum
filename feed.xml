<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog Name</title>
  <subtitle>Blog subtitle</subtitle>
  <id>http://blog.url.com/modules</id>
  <link href="http://blog.url.com/modules"/>
  <link href="http://blog.url.com/feed.xml" rel="self"/>
  <updated>2014-09-18T20:00:00-04:00</updated>
  <author>
    <name>Blog Author</name>
  </author>
  <entry>
    <title>Save Results to File</title>
    <link rel="alternate" href="http://blog.url.com/modules/module13-save.html"/>
    <id>http://blog.url.com/modules/module13-save.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-11T20:12:51-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">
</content>
  </entry>
  <entry>
    <title>Analyzing a Subset of the Data</title>
    <link rel="alternate" href="http://blog.url.com/modules/module12-analyze.html"/>
    <id>http://blog.url.com/modules/module12-analyze.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-11T23:38:54-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;// Code we need to get them to
from dpla.api import DPLA
import json
from collections import Counter
from nltk.corpus import stopwords&lt;/p&gt;

&lt;p&gt;description_words = []
stop = stopwords.words('english')&lt;/p&gt;

&lt;p&gt;with open("cooking_results.txt") as json_file:
	json_data = json.load(json_file)&lt;/p&gt;

&lt;p&gt;print json.dumps(json_data[1], sort_keys=True, indent=4, separators=(',', ': '))&lt;/p&gt;

&lt;p&gt;def get_words():
	for each in json_data:
		try:
			description = each['sourceResource']['description']
			words = description.split()
			for each in words:
				if each.lower() not in stop:
					if not each.isdigit():
						description_words.append(each.lower())&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	except:
		print "no description"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;get_words()&lt;/p&gt;

</content>
  </entry>
  <entry>
    <title>Gathering a Subset of the Data</title>
    <link rel="alternate" href="http://blog.url.com/modules/module11-subset.html"/>
    <id>http://blog.url.com/modules/module11-subset.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-12T00:11:19-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;In this module, we will learn how to collect and store the description data from each item in our "json_data" variable. To do this, we will first identify the location of the description with the data structure and then loop through each item, saving each of the words in the description into a new list.&lt;/p&gt;

&lt;h3 id="identifying-our-target-fields"&gt;Identifying Our Target Fields&lt;/h3&gt;

&lt;p&gt;Remember back to JSON and how it organizes data using key:value pairs? One of the most powerful features of JSON is that we are able to nest features and create lists within key:value pairs. This is useful for creating complex data structures. It also means that we have work within the hierarchy of key:value pairs to isolate particular values.&lt;/p&gt;

&lt;p&gt;In order to better see that hierarchy, let's &lt;strong&gt;Pretty Print&lt;/strong&gt; the JSON results, or print with the indentations and hierarchies visually displayed. &lt;/p&gt;

&lt;p&gt;Open "my_second_script.py" and &lt;strong&gt;comment out&lt;/strong&gt; "print jason_data[0]" by putting # at the beginning of the line. When your computer executes the file, it will skip all lines that start with a pound sign. This allows you to leave comments for yourself or to test new ways of doing things without loosing your work.&lt;/p&gt;

&lt;p&gt;At the bottom of your script, add the line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print json.dumps(json_data[0], sort_keys=True, indent=4, separators=(',', ': '))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save and run in Terminal. &lt;/p&gt;

&lt;p&gt;Work with your table to map out what this command did.&lt;/p&gt;

&lt;p&gt;We now need to find the "description" field within the json object. You can either read through all of the lines of code or you can search within Terminal by pressing "command F". &lt;/p&gt;

&lt;p&gt;Looking at the "description" field, work with your table to identify the field name it is nested under. &lt;/p&gt;

&lt;p&gt;The syntax for calling items within a data structure uses brackets to designate the key names or the position in the list of the item we want to interact with. We already use this syntax when we call the first item in the data by using &lt;span class="command"&gt;json_data[0]&lt;/span&gt;. This form means we want the first item in the json_data object. &lt;/p&gt;

&lt;p&gt;If we want the value associated with the "description" key, we use a similar format. &lt;/p&gt;

&lt;p&gt;To print only the description for the first item in our list, add the following to the bottom of your "my_second_script.py" file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print json_data[0]['sourceResource']['description']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Work with your table to print only the titles. What about only the subject headings?&lt;/p&gt;

&lt;h3 id="saving-the-descriptions"&gt;Saving the Descriptions&lt;/h3&gt;

&lt;p&gt;One way to look at the descriptions is to consider them as a collection, or list, of words. If we take all of the collections as a whole, we can ask if there are any noticable patterns in the words being used to describe these items related to cooking.&lt;/p&gt;

&lt;p&gt;One common way of dealing with text computationally is to translate the text into a "bag of words". In a bag of words approach, the order does not matter. What we are looking at instead is the collection of words and their frequencies.&lt;/p&gt;

&lt;p&gt;To create a "bag of words" from the descriptions we need to 1) create a list to store the words and 2) write a loop that takes every description and adds the words to the list.&lt;/p&gt;

&lt;p&gt;See if you can use the information in module 7 (functions and loops) to create an empty list.&lt;/p&gt;

&lt;p&gt;Currently our "my_second_script.py" file should look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import json
from collections import Counter
import nltk
from nltk.corpus import stopwords

with open("search_results.json") as json_file:
  json_data = json.load(json_file)
	
#print json_data[0]
#print json.dumps(json_data[0], sort_keys=True, indent=4, separators=(',', ': '))

print json_data[0]['sourceResource']['description']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I will use "description_words" as the variable for my empty list of stopwords. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;description_words = []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we will create a new function called "get_words"&lt;/p&gt;

&lt;p&gt;Declare the function by adding:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, tab in once and add a for-loop to move through each item in the json_data list:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, let's set up a variable to grab the descriptions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
    descriptions = each['sourceResource']['description']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, description holds the full sentence as a single object and we want to get the individual words. We can use a built in function called "split" that divides the sentence into an array of individual objects, splitting on the spaces and punctuation marks.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
    descriptions = each['sourceResource']['description']
    for line in descriptions:
      words = line.split()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To see what just happened, let's add a print statement and call the function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
    descriptions = each['sourceResource']['description']
    for line in descriptions:
      words = line.split()
      print words

get_words()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your file should now look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import json
from collections import Counter
import nltk
from nltk.corpus import stopwords

with open("search_results.json") as json_file:
  json_data = json.load(json_file)
	
#print json_data[1]
#print json.dumps(json_data[0], sort_keys=True, indent=4, separators=(',', ': '))

print json_data[0]['sourceResource']['description']

description_words = []

def get_words():
  for each in json_data:
    descriptions = each['sourceResource']['description']
    for line in descriptions:
      words = line.split()
      print words

get_words()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save the file and run it in the terminal.&lt;/p&gt;

&lt;p&gt;You might first notice that this script threw an error, saying:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KeyError: 'description'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One of the most important things in learning to work with code is learning to decipher error messages. If you read a little more in your terminal window, you'll see that the full message is something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jeriwieringa$ python my_second_script.py
[u'This book is published to give greater publicity to the exceptional qualities of Crisco."--Introd.']
[u'This', u'book', u'is', u'published', u'to', u'give', u'greater', u'publicity', u'to', u'the', u'exceptional', u'qualities', u'of', u'Crisco."--Introd.']
Traceback (most recent call last):
  	File "my_second_script.py", line 21, in &amp;lt;module&amp;gt;
	get_words()
  	File "my_second_script.py", line 16, in get_words
	descriptions = each['sourceResource']['description']
KeyError: 'description'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, we got a print out of the description sentence and then the words of that description as a list. This is good!&lt;/p&gt;

&lt;p&gt;Then we got a traceback saying in our "get_words()" function there was an error on line 16 where we told it to save the value associated with the "description" key to the variable "descriptions". &lt;/p&gt;

&lt;p&gt;Let's see what is going. Comment out the call to "get_words" function.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# get_words()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let's print the second object in the json_data list. Uncomment our print statement at the beginning of the file that pretty-prints the json. Change the 0 to 1, the save and run the file.&lt;/p&gt;

&lt;p&gt;Search for 'description' under 'sourceResource'. If you can't find it, it's because this item appears to be missing "description" information. Because the computer can't find the description, it returns an error and stops.&lt;/p&gt;

&lt;p&gt;Getting around this problem requires what is called "handling exceptions". We had expected a particular pattern in our data (that there was "description" keys nested under "sourceResource" keys) but we've encountered an exception to that general pattern. &lt;/p&gt;

&lt;p&gt;To tell the computer to keep moving if it encounters an item it cannot parse because of a KeyError, we will adjust our get_words() function as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
    try:
      descriptions = each['sourceResource']['description']
      for line in descriptions:
      words = line.split()
        print words
    except KeyError:
      print "Description Missing"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Un-comment "get_words()", save, and run the file.&lt;/p&gt;

&lt;p&gt;Our script should no longer error.&lt;/p&gt;

&lt;p&gt;But, it seems that sometime we are getting a list of letters, and sometimes a list of words. So one more adjustment is in order. We need to perform a check, where if the value of descriptions is a string, the function does one thing, and if it is not a string, it does something else. Our current code works well on the lists, so we will keep that for not strings.&lt;/p&gt;

&lt;p&gt;To check if the value in "descriptions" in a string, we can add the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
    try:
    descriptions = each['sourceResource']['description']
    if isinstance(descriptions, basestring):
      words = descriptions.split()
    else:
      for line in descriptions:
        words = line.split()
        print words
  except KeyError:
    print "Description Missing"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looking through the information printing to our terminal window, it looks like we have correctly split out all of the words.&lt;/p&gt;

&lt;h3 id="adding-the-words-to-our-bag-of-words"&gt;Adding the words to our "bag of words"&lt;/h3&gt;

&lt;p&gt;The final step in creating our subset of data is to add the words into our "description_words" list.&lt;/p&gt;

&lt;p&gt;To do this, we will need to loop through the "words" list and add each word to the "description_words" list. This means another for-loop within the "try" second of our function.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
for each in json_data:
  try:
    descriptions = each['sourceResource']['description']
    if isinstance(descriptions, basestring):
      words = descriptions.split()
    else:
      for line in descriptions:
        words = line.split()
    print words

    for word in words:
      description_words.append(word)

  except KeyError:
    print "Description Missing"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now to check that everything worked ok, let's run the function and print out the 1000 word in our "descriptions_words"&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import json
from collections import Counter
import nltk
from nltk.corpus import stopwords

with open("search_results.json") as json_file:
json_data = json.load(json_file)
	
#print json_data[1]
#print json.dumps(json_data[0], sort_keys=True, indent=4, separators=(',', ': '))

#print json_data[0]['sourceResource']['description']

description_words = []

def get_words():
for each in json_data:
  try:
    descriptions = each['sourceResource']['description']
    if isinstance(descriptions, basestring):
      words = descriptions.split()
    else:
      for line in descriptions:
        words = line.split()
    print words

    for word in words:
      description_words.append(word)

  except KeyError:
    print "Description Missing"
  
get_words()
print description_words[999]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bonus Challenges: &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Can you modify the function to also add the words of the title to our bag of words?&lt;/li&gt;
  &lt;li&gt;Can you also add the subject headings?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="what-we-learned"&gt;What We Learned&lt;/h3&gt;

&lt;p&gt;In this module, we learned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;to pretty-print json with Python&lt;/li&gt;
  &lt;li&gt;to identify the nested key:value pairs we want to analyze&lt;/li&gt;
  &lt;li&gt;to split a list &lt;/li&gt;
  &lt;li&gt;to trouble shoot from an error report&lt;/li&gt;
  &lt;li&gt;to handle exceptions&lt;/li&gt;
  &lt;li&gt;to check type and use an "if/else" statement&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry>
    <title>Working with Local Data</title>
    <link rel="alternate" href="http://blog.url.com/modules/module10-local_data.html"/>
    <id>http://blog.url.com/modules/module10-local_data.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-12T00:16:04-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;Now that we have a very large file of JSON data, we can work locally to find  patterns that we could not find using the online interface for the DPLA's holdings. &lt;/p&gt;

&lt;p&gt;In this module, we will create a new script and learn how to load data from our "search_results.json" file. We will also install the Natural Language ToolKit library in order to do some text analysis on our data.&lt;/p&gt;

&lt;h3 id="loading-from-a-local-file"&gt;Loading from a Local File&lt;/h3&gt;

&lt;p&gt;It is time to create a new script file!&lt;/p&gt;

&lt;p&gt;Go to terminal and type:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;touch my_second_script.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To open your new script file, type:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;open my_second_script.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To begin, let's load in some of the libraries we will need for this next phase of the workshop.&lt;/p&gt;

&lt;p&gt;First, we will need the json library again. To add this, type &lt;span class="command"&gt;import json&lt;/span&gt; at the very top of the file.&lt;/p&gt;

&lt;p&gt;We are also going to use a library called "collections" and a specific part of that library called "Counter". To import this, type &lt;span class="command"&gt;from collections import Counter&lt;/span&gt; on the next line.&lt;/p&gt;

&lt;p&gt;The next thing is to load up the data from our "search_results.json" file.&lt;/p&gt;

&lt;p&gt;The structure for this is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;with open("search_results.json") as json_file:
	json_data = json.load(json_file)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, we tell Python to "open" our "search_results.json" file and assign it to the variable "json_file". The we use the "load" method in the "json.library" to load up the data and save it as the variable "json_data". &lt;/p&gt;

&lt;p&gt;To make sure this worked, let's print out one item from the json data. Work with our table to add a print statement to print the first item in the json_data list.&lt;/p&gt;

&lt;h3 id="installing-nltk"&gt;Installing NLTK&lt;/h3&gt;

&lt;p&gt;We are interested in the languaged used in the "description" fields across all the "cooking" items in the DPLA database. Fortunately, there is good support within Python for text analysis and one power library we can use is the Natural Language ToolKit (or NLTK).&lt;/p&gt;

&lt;p&gt;To install NLTK, let's go back to our terminal and use pip.&lt;/p&gt;

&lt;p&gt;Run &lt;span class="command"&gt;pip install nltk&lt;/span&gt;. You may need to use &lt;span class="command"&gt;sudo pip install nltk&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Now, go back to your file and import nltk at the top of the file.&lt;/p&gt;

&lt;p&gt;There are also a number of datasets available for use with nltk. For our purposes, we will only be using the 'stopwords' dataset, but browse the list of all the datasets you could download and use at &lt;a href="http://www.nltk.org/nltk_data/"&gt;http://www.nltk.org/nltk_data/&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;To download the stopwords, we are going to go back into the Python Interactive Shell. Run &lt;span class="command"&gt;python&lt;/span&gt;. Your terminal window should now look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Python 2.7.5 (default, Mar  9 2014, 22:15:05)
[GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
&amp;gt;&amp;gt;&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Type &lt;span class="command"&gt;import nltk&lt;/span&gt; and press enter.&lt;/p&gt;

&lt;p&gt;Next type &lt;span class="command"&gt;nltk.download('stopwords')&lt;/span&gt; and press enter.&lt;/p&gt;

&lt;p&gt;Great!&lt;/p&gt;

&lt;p&gt;Now we need to load these stopwords into our Python file. Switching back to our "my_second_script.py" file, add &lt;span class="command"&gt;from nltk.corpus import stopwords&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;We are now ready to start working with our data.&lt;/p&gt;

&lt;h3 id="what-we-learned"&gt;What We Learned&lt;/h3&gt;

&lt;p&gt;In this module, we have reviewed and learned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;to create new files using terminal&lt;/li&gt;
  &lt;li&gt;to load json data from a file &lt;/li&gt;
  &lt;li&gt;to download libraries using pip&lt;/li&gt;
  &lt;li&gt;to download datasets within libraries&lt;/li&gt;
  &lt;li&gt;to load libraries into our scripts&lt;/li&gt;
&lt;/ul&gt;

</content>
  </entry>
  <entry>
    <title>Writings Search Results to a File</title>
    <link rel="alternate" href="http://blog.url.com/modules/module09-write.html"/>
    <id>http://blog.url.com/modules/module09-write.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-12T00:16:01-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;In this module, we will focus on writing the results of our functions to a text file. This gives us a local copy of the data so that we only hit the DPLA servers once for the entire collection of files.&lt;/p&gt;

&lt;h3 id="opening-a-new-file"&gt;Opening a new file&lt;/h3&gt;

&lt;p&gt;Our first step is to create and open a new file that we will save our search results to. In your "my_first_script.py" file, right after you declare the "all_records" variable, add:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f = open("search_results.json", "w")
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here you are combining a function - open("search_results.json") - with the declaration of a variable. "Open" also creates a file if the file does not already exist on your computer. The "w" indicates that the file should be opened as "write". One thing to note about "w" - "write" gives the computer permission to overwrite the data inside the file, which is why we are opening the file once and writing the whole array at the end. If we were to write each item as we looped, we would end up with only the last item in the file. It would write and overwrite each item as it went along. If you need to write inside a loop, you can open the file as "a". This tells the computer to "append" the information to the end of the file, rather than overwrite the existing information. &lt;/p&gt;

&lt;h3 id="creating-a-write-function"&gt;Creating a "Write" Function&lt;/h3&gt;

&lt;p&gt;Now that we have a file to write to, let's create a third function to write our search results to the file. You can put this after &lt;span class="command"&gt;f = open("search_results.json", "w")&lt;/span&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def save_results():
	data = json.dumps(all_records)
	f.write(data)
	f.close
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, we are creating a variable called "data". Working with json objects in python requires the use of another library, called "json", and json.dumps is an operation that helps python know how to interact with all the data in "all_records". Now, because we are using a new library, we also need to import that library at the beginning of the file. Add &lt;span class="command"&gt;import json&lt;/span&gt; to the top of the file. Next, we are taking our file variable from the first part of the module and writing all of the information contained in "data" to it. We then close the file.&lt;/p&gt;

&lt;h3 id="calling-the-write-function"&gt;Calling the "Write" Function&lt;/h3&gt;

&lt;p&gt;To run the "save_results" function, we can now call the function at the end of our file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;save_results()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our file should now look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from dpla.api import DPLA
import json

dpla = DPLA('Your-Key-Here')

# result = dpla.search('cooking')
# print result.items[1]
	
all_records = []
f = open('search_results.json', 'w')

def save_results():
	data = json.dumps(all_records)
	f.write(data)
	f.close

def pull_records(pages, end, size):
	while(pages &amp;lt;= end):
		paged_search = dpla.search(q='cooking', page_size=size, page=pages)
		save_each(paged_search)
		print "finished page " + str(pages)
		pages = pages + 1

def save_each(n):
	for each in n.items:
		all_records.append(each)

pull_records(2, 5, 50)

print all_records[150]

save_results()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Test that everything is working by running your "my_first_script.py" in terminal.
You can open the "search_results.json" file to check that 200 items made it in.&lt;/p&gt;

&lt;h3 id="saving-all-the-search-results"&gt;Saving all the Search Results&lt;/h3&gt;

&lt;p&gt;Now, let's change the parameters we pass to the "pull_records" function to get all of the search results.&lt;/p&gt;

&lt;p&gt;Remember, the number we pass to "pull_records" is the first page, the second number is the last page, and the third number is the number of items per page. The DPLA will cap us at 500 items per page, so let's take '500' for our third variable. We also want to start with the first page, so '1' is our first variable.&lt;/p&gt;

&lt;p&gt;To figure out the value we want for "end", we need to do a little math. If we have 10,909 items and can get 500 items a page, how many pages do we have to work through?&lt;/p&gt;

&lt;p&gt;Update your "pull_records" line to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pull_records(1, 22, 500)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save and run your script. This will take a bit of time, so get up, stretch, and get some coffee!&lt;/p&gt;

&lt;p&gt;Bonus Challege: Can you write a function that determines the total number of pages from the search results?&lt;/p&gt;

&lt;h3 id="what-we-learned"&gt;What We Learned&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;To create and open a new file&lt;/li&gt;
  &lt;li&gt;The difference between "write" and "append" as ways of opening a file&lt;/li&gt;
  &lt;li&gt;To write json to a file&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry>
    <title>Looping through the Pages</title>
    <link rel="alternate" href="http://blog.url.com/modules/module08-whileloop.html"/>
    <id>http://blog.url.com/modules/module08-whileloop.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-11T16:54:00-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;In this module, we will add another loop to our "pull records" function that allows us to move through more than one page of search results.&lt;/p&gt;

&lt;h3 id="introducing-the-while-loop"&gt;Introducing the While Loop&lt;/h3&gt;

&lt;p&gt;The "for loop" allows us to do something to each item in a list. The "while loop" is a powerful tool that tells the computer to continue doing something as long as some criteria is true. We can use the while loop and a "counter" to work through all of the pages of search results.&lt;/p&gt;

&lt;p&gt;To use a while loop, let's look again at our "pull records" function.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def pull_records(pages, end, size):
	paged_search = dpla.search(q='cooking', page_size=size, page=pages)
	save_each(paged_search)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remember, "pages" stands for the first page and "end" stands for the last page of search results we want. We want this function to run for every page of search results. In other words, if the page number is less than or equal to the total number of pages available, we want to get the search results from that page. Once we hit the end, we want to stop.&lt;/p&gt;

&lt;p&gt;To write this logic in code, we will add:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while(pages &amp;lt;= end):
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;so that our function now looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def pull_records(pages, end, size):
	while(pages &amp;lt;= end):
		paged_search = dpla.search(q='cooking', page_size=size, page=pages)
		save_each(paged_search)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id="adding-a-counter"&gt;Adding a Counter&lt;/h3&gt;

&lt;p&gt;Can you see the problem with our current function? As it currently stands, "pages" is always less than "end" because it never increases. This means we would get stuck in an "infinite loop" if we tried to run the code right now.&lt;/p&gt;

&lt;p&gt;To avoid the infinite loop, we need to increase the value of "pages" each time we work through the loop. We can do this by overwriting the value of "pages" to be "pages + 1".&lt;/p&gt;

&lt;p&gt;After &lt;span class="command"&gt;save_each(paged_search)&lt;/span&gt; add:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pages = pages + 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let's also add a print command to check that things are working as we expect. Above &lt;span class="command"&gt;pages = pages + 1&lt;/span&gt; add:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print "finished page " + str(pages)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our file should now look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from dpla.api import DPLA

dpla = DPLA('Your-Key-Here')

# result = dpla.search('cooking')
# print result.items[1]

all_records = []

def pull_records(pages, end, size):
	while(pages &amp;lt;= end):
		paged_search = dpla.search(q='cooking', page_size=size, page=pages)
		save_each(paged_search)
		print "finished page " + str(pages)
		pages = pages + 1

def save_each(n):
	for each in n.items:
		all_records.append(each)

pull_records(2, 3, 50)

print all_records[40]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let's test our function on a subset of the pages. Change &lt;span class="command"&gt;pull_records(2, 3, 50)&lt;/span&gt; to &lt;span class="command"&gt;pull_records(2, 5, 50)&lt;/span&gt; and change &lt;span class="command"&gt;print all_records[40]&lt;/span&gt; to &lt;span class="command"&gt;print all_records[150]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Save and run in Terminal.&lt;/p&gt;

&lt;h3 id="what-we-learned"&gt;What We Learned:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;To use a while loop and counter&lt;/li&gt;
  &lt;li&gt;To test on a subset of the data&lt;/li&gt;
  &lt;li&gt;To use "print" to check our functions along the way&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
</feed>
