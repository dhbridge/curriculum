<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog Name</title>
  <subtitle>Blog subtitle</subtitle>
  <id>http://blog.url.com/modules</id>
  <link href="http://blog.url.com/modules"/>
  <link href="http://blog.url.com/feed.xml" rel="self"/>
  <updated>2014-10-06T20:00:00-04:00</updated>
  <author>
    <name>Blog Author</name>
  </author>
  <entry>
    <title>Overview for Coaches</title>
    <link rel="alternate" href="http://blog.url.com/modules/coaches-guide.html"/>
    <id>http://blog.url.com/modules/coaches-guide.html</id>
    <published>2014-10-06T20:00:00-04:00</published>
    <updated>2014-10-31T15:39:46-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;Welcome to being a coach for a DH Bridge workshop! This document will orient you to the goals and objectives for the day (and beyond), how the tutorials have been scaffolded, and the places where your expertise will be needed to guide participants. &lt;/p&gt;

&lt;h3 id="goals-and-objectives"&gt;Goals and Objectives&lt;/h3&gt;

&lt;p&gt;Coaches will need to: &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Be mindful of the obstacles that make it difficult for persons from underrepresented groups to learn to code, and be respectful of particpants' efforts to learn.&lt;/li&gt;
  &lt;li&gt;Foster collegial interaction between participants in their group, and encourage participants to collaborate and help each other when possible. &lt;/li&gt;
  &lt;li&gt;Facilitate participants' forays into computational thinking by highlighting the relationships between data, content, and questions.&lt;/li&gt;
  &lt;li&gt;Encourage participants to avoid self-deprecating/apologetic language (I'm not sure if I'm smart enough, I'm not very good with technology, etc.). &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="tips-for-the-day"&gt;Tips for the Day&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Not all questions have solutions, errors will happen, and that's ok. Talk through participants' results, and move them toward forming their own questions to troubleshoot.&lt;/li&gt;
  &lt;li&gt;If a group is progressing quickly through the tutorials, have them explain back what the different lines of code are doing and/or complete the Bonus Challenges on the relevant tutorials. &lt;/li&gt;
  &lt;li&gt;There shouldn't be any copying/pasting of the code.&lt;/li&gt;
  &lt;li&gt;Only 1 folder should be used for the entire workshop.&lt;/li&gt;
  &lt;li&gt;Encourage participants to add their own commments in the script files along the way.&lt;/li&gt;
  &lt;li&gt;Ask before taking over a participant's machine to demonstrate or fix a problem, and only do so if absolutely necessary. &lt;/li&gt;
  &lt;li&gt;To make things easy on everyone, the following terminology will be used consistently throughout the day:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Terminal and Powershell = Terminal
Directories and Folders = Folders 
For Mac/Windows term translation: &lt;a href="http://www.dummies.com/how-to/content/comparing-common-windows-terms-with-mac-terms.html"&gt;http://www.dummies.com/how-to/content/comparing-common-windows-terms-with-mac-terms.html&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Capitalization is crucial in Windows Powershell, so if you see participants running into problems it's always a good idea to check there. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All complete script and result files are available for your reference here: &lt;a href="http://curriculum.dhbridge.org/code/"&gt;http://curriculum.dhbridge.org/code/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id="full-day-schedule"&gt;Full Day Schedule&lt;/h3&gt;

&lt;p&gt;Note: the timeblocks are suggestions and will depend on the progress of your group. The first half of the day is primarily concerned with introducing thinking computationally: how a computer processes information locally and with other computers. The second half of the day builds on this foundation to address computational thinking about humanities data: how to utilize computation to ask questions and work with the kinds of data that interest humanities scholars.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;9:30-10:20a&lt;/strong&gt; Welcome and Setting Up the Learning Environment: goals and objectives for the day, introductions within small groups. Coaches' Project Demonstrations: current projects and problems&lt;/p&gt;

&lt;p&gt;10 Minute Break&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;10:30-Noon:&lt;/strong&gt; Modules 1-5&lt;/p&gt;

&lt;p&gt;LUNCH&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1:00-2:30p:&lt;/strong&gt; Modules 6 - 8&lt;/p&gt;

&lt;p&gt;10 Minute Break&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2:40-4:00p:&lt;/strong&gt; Modules 9 - 11&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4:00-5:00p:&lt;/strong&gt; Modules 12 - 13&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5:00-5:30:&lt;/strong&gt; Wrapup (recap the day and tips for continued learning)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5:30:&lt;/strong&gt; Decompressing and socializing&lt;/p&gt;

&lt;h3 id="tutorial-modules"&gt;Tutorial Modules&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#one"&gt;Module 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#two"&gt;Module 2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#three"&gt;Module 3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#four"&gt;Module 4&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#five"&gt;Module 5&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#six"&gt;Module 6&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#seven"&gt;Module 7&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#eight"&gt;Module 8&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#nine"&gt;Module 9&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#ten"&gt;Module 10&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#eleven"&gt;Module 11&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#twelve"&gt;Module 12&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="#thirteen"&gt;Module 13&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="installation"&gt;Installation&lt;/h3&gt;

&lt;p&gt;Check to make sure that participants have the following installed and ready to go:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Python 2.7&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;pip&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Text Editor: work with participants to set TextWrangler as the default text editor for Mac users. It's not an issue for Windows users because the commands specify starting Notepad++ each time. &lt;/li&gt;
  &lt;li&gt;Chrome browser&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If possible, please have participants pin the text editor and Chrome to their Dock (Mac)/Taskbar (Windows), or at least have them easily accessible. &lt;/p&gt;

&lt;h3 id="one"&gt;One&lt;/h3&gt;

&lt;p&gt;(To be done with the full group)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;After your group members have all created their "dhb_awesome" folder have them find where it lives on their computer through Finder/My Computer. Also have them find their newly created "my_first_script.py" file. Have them compare the directory path with what they did in Terminal to ensure they can see that they are creating files and folders on their local machine.   &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="two"&gt;Two&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Module Goal:&lt;/strong&gt; to understand how computers talk to one another (through the syntax of API requests) and how computers display data (as a data dump vs. JSON).&lt;/p&gt;

&lt;p&gt;Beginning Activity: 
Start with the DPLA interface in the browser and have your group search for "cooking". Have your group read what's going on in the URL. Filter the number of items displayed using the GUI's filters. How did the URL change? What's the syntax?  &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Have them talk through the API request URL to reinforce understanding of the structure of the request.&lt;/li&gt;
  &lt;li&gt;Review how JSON stores data items as objects and how it displays those objects.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="three"&gt;Three&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Module Goal:&lt;/strong&gt; to understand the benefits of installing libraries via &lt;code&gt;pip&lt;/code&gt;, and the relationship between the Python Interactive Shell and Terminal in the local machine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt; &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Make sure participants are clear on difference between Python Interactive Shell and Terminal.&lt;/li&gt;
  &lt;li&gt;Terminology check: string, variable, array/list&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="four"&gt;Four&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Module Goal:&lt;/strong&gt; to understand how to create targeted queries for data and generally the different kinds of questions that can be pursued/answered based on those queries.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Constructing additional queries: highlight the questions that would motivate the choice of different commands and filtering of results.&lt;/li&gt;
  &lt;li&gt;See if there are any general questions about APIs and the documentation used in the module. &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="five"&gt;Five&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Module Goal:&lt;/strong&gt; to understand how programming languages interact with computers and the basic function and purpose of libraries.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mapping Exercise #1:&lt;/strong&gt; have participants diagram the following, similar to the API exercise from earlier in the day:
-High-level programming languages (like Python) enable you to write commands for your computer in something that approximates English. Those commands are then translated down to machine language, executed by the hardware, and the results are translated back to generate the desired output. Your computer is constantly processing commands from the applications on your machine in multiple programming languages. Just like those applications, you can use the terminal interface to send commands to your computer.
-Discuss everyone's diagrams and make sure the concepts are clear.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mapping Exercise #2:&lt;/strong&gt; have participants diagram the following, similar to the API exercise from earlier in the day:
-A Python module or library is a bundle of code, including variables and functions (defined processes), that does a particular task. Many Python modules already exist out in the world, ready to be used, and new ones are developed by programmers all the time. Python scripts, which combine these modules with additional python commands, give the computer new, and more complicated, tasks that can be completed.
-Discuss everyone's diagrams and make sure the concepts are clear.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;More on libraries:  they are bundles of code that we can combine to do different tasks - we don’t have to write everything new every time. It’s like cake-mix or bottled sauces (but without the cultural baggage) - by combining the mixes with other ingredients, we can create new things very quickly.&lt;/p&gt;

&lt;h3 id="six"&gt;Six&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Module Goal:&lt;/strong&gt; how to begin focusing on the data relevant to the research questions at hand and selecting the relevant aspects to further pursue those questions.&lt;/p&gt;

&lt;p&gt;Before your group jumps into the code-heavy part of the module, start off with the quick review of how script files should be organized. There will also be a handout for participants to reference as they continue through the modules. Participants may be tempted to gloss over this part, but make sure they take the time to get the format down. 
1. load libraries
2. set variables to be used throughout the script
3. define all functions
4. make calls/commands to execute the functions&lt;/p&gt;

&lt;p&gt;Optional Activity: have participants create their own mnemonic device for the above format (LSDM)&lt;/p&gt;

&lt;p&gt;Terminology related to functions: from now on the tutorials will heavily use new phrases to describe how the functions work in relation to the data. These phrases are part of programming vernacular, and so they're not jarring to participants, here are some basic definitions to maintain consistency:
1. call: to execute 
2. declare/set: create a new function
3. pass: to run data through the function/loop&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt;
1. Have a quick discussion about the merits of commenting out versus deleting lines
2. Verify that participants understand setting variables and how they can hold lists (an empty list so far).&lt;/p&gt;

&lt;h3 id="seven"&gt;Seven&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt; 
1. Review and explain in a group discussion: how functions can be combined to solve problems, with the example of the two functions written so far and how they work together.
2. Review what "append" does.
3. Group challenge: talk through the functionality and uses for a "for loop"&lt;/p&gt;

&lt;h3 id="eight"&gt;Eight&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt; 
1. Group challenge: along with talking through how a "while loop" works, also review why the "while loop" is added into the function where it is. &lt;/p&gt;

&lt;h3 id="nine"&gt;Nine&lt;/h3&gt;
&lt;p&gt;Questions for the participants to discuss/keep in mind throughout the module:
1. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt;
1. The value of working locally: writing the search results to a JSON file gives the flexibility for working with a large collection of files on a local machine without having to constantly hit the DPLA (or any other API servers) and to find patterns that we could not find using the online interface for the DPLA's holdings.
2. When to use "append": writing inside a loop, which allows the comupter to add the information it grabs to the end of the file rather than overwriting what's there each time. &lt;/p&gt;

&lt;h3 id="ten"&gt;Ten&lt;/h3&gt;
&lt;p&gt;Questions for the participants to discuss/keep in mind throughout the module: 
1. Are there other data fields to include that would help address the research question? 
2. What other research questions can you ask of this data?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt;
1. Dealing with the realities of messy data: pristine datasets are extremely rare. In our example, the "cooking" data features many items that don't have information for the three fields that we're focusing on. 
2. Open your "text_results.txt" file in TextWrangler/Notepad++ and scroll through what was saved. What do you notice? What information is/isn't there? What kinds of hypotheses can you make based on these results?&lt;/p&gt;

&lt;p&gt;**We anticipate this will be as far as participants can get in one day without getting too frustrated or burned out. If that's the case, encourage them to talk through what they've already done, clarify parts that are still mysterious/confusing, and/or go back to the earlier modules and work through with different search terms and parameters. They can also do the activity posed as a Learning Check in Module 13.&lt;/p&gt;

&lt;h3 id="eleven"&gt;Eleven&lt;/h3&gt;
&lt;p&gt;Questions for the participants to discuss/keep in mind throughout the module:
1. How is the script parsing the text data? 
2. What are the benefits and limitations of breaking the data into chunks in this way?
3. How would you speak to the ways you as the researcher have shaped this dataset to a colleague?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt; 
1. Words and tokens: what is NLTK considering a "word"? what is a token? 
2. Collocations, concordances, and "similar": what are they doing? what analytical value does each have for the research question at hand and for your own work? &lt;/p&gt;

&lt;h3 id="twelve"&gt;Twelve&lt;/h3&gt;
&lt;p&gt;Questions for the participants to discuss/keep in mind throughout the module: 
1. How does the results generated by the text mining script compare to your own hypotheses? 
2. What kinds of research projects and questions benefit from this approach? &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt;
1. Stopwords: what makes up these lists? how does one need to account for different stopwords based on the research at hand, language, corpus, etc.?
2. Encourage participants to checkout the documentation for &lt;a href="http://www.nltk.org/"&gt;NLTK&lt;/a&gt; to get a better sense of what else it can do.  &lt;/p&gt;

&lt;h3 id="thirteen"&gt;Thirteen&lt;/h3&gt;
&lt;p&gt;Questions for the participants to discuss/keep in mind throughout the module:
1. What are the benefits of having the data in a CSV file?
2. What else can you do with the data that's now in the CSV file? &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Checks:&lt;/strong&gt;
1. Have participants pull out their diagrams of Terminal, API, and programming languages and make any necessary revisions based on what they've learned throughout the day. &lt;/p&gt;

</content>
  </entry>
  <entry>
    <title>Looping through the Pages</title>
    <link rel="alternate" href="http://blog.url.com/modules/module08.html"/>
    <id>http://blog.url.com/modules/module08.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-31T21:52:40-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;In this module, we will learn to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;use a "while loop"&lt;/li&gt;
  &lt;li&gt;create a "counter"&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We're getting there! We have a function to gather all of the search results on any given page. We now need a way to work systematically through all of the pages available.&lt;/p&gt;

&lt;p&gt;To do that, we will add another loop to our "&lt;code&gt;pull_records&lt;/code&gt;" function that allows us to move through more than one page of search results.&lt;/p&gt;

&lt;h3 id="introducing-the-while-loop"&gt;1. Introducing the &lt;code&gt;while&lt;/code&gt; Loop&lt;/h3&gt;

&lt;p&gt;The "for loop" allows us to do something to each item in a list. The "while loop" is a powerful tool that tells the computer to continue doing something as long as some criteria is true. We can use the "while loop" and a "counter" to work through all of the pages of search results.&lt;/p&gt;

&lt;p&gt;To use a &lt;code&gt;while&lt;/code&gt; loop, let's look again at our "&lt;code&gt;pull_records&lt;/code&gt;" function.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;def pull_records(pages, end, size):
	paged_search = dpla.search('cooking',fields=['sourceResource'], page_size=size, page=pages)
	# print paged_search.items[2]
	save_each(paged_search)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Remember, "&lt;code&gt;pages&lt;/code&gt;" is a variable for the first page and "&lt;code&gt;end&lt;/code&gt;" is a variable for the last page of search results we want. We want the &lt;code&gt;pull_records&lt;/code&gt; function to retrieve every page of search results. In other words, if the page number is less than or equal to the total number of pages available, we want to get the search results from that page. Once we hit the end, we want to stop.&lt;/p&gt;

&lt;p&gt;To write this logic into our existing function code, we will add:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;while(pages &amp;lt;= end):
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;so that our function now looks like this:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;def pull_records(pages, end, size):
	while(pages &amp;lt;= end):
		paged_search = dpla.search('cooking',fields=['sourceResource'], page_size=size, page=pages)
		# print paged_search.items[2]
		save_each(paged_search)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Can you see the problem with our current function? As it currently is written, "&lt;code&gt;pages&lt;/code&gt;" is always less than "&lt;code&gt;end&lt;/code&gt;" because it never increases. This means we would get stuck in an "&lt;strong&gt;infinite loop&lt;/strong&gt;" if we tried to run the code right now — don't run the code at this point! If you did, that's OK, just stop the execution of the program with by hitting &lt;code&gt;ctrl + c&lt;/code&gt; on the keyboard.&lt;/p&gt;

&lt;p&gt;To avoid an "&lt;strong&gt;infinite loop&lt;/strong&gt;", we need to increase the value of "&lt;code&gt;pages&lt;/code&gt;" each time we work through the loop. We can do this by overwriting the value of "&lt;code&gt;pages&lt;/code&gt;" to be "&lt;code&gt;pages + 1&lt;/code&gt;".&lt;/p&gt;

&lt;p&gt;Add "pages = pages +1" after &lt;span class="command"&gt;save_each(paged_search)&lt;/span&gt; like this:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;def pull_records(pages, end, size):
		while(pages &amp;lt;= end):
			paged_search = dpla.search('cooking',fields=['sourceResource'], page_size=size, page=pages)
			# print paged_search.items[2]					
			save_each(paged_search)
			pages = pages + 1
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Let's also add a print command to check that things are working as we expect. Above &lt;span class="command"&gt;pages = pages + 1&lt;/span&gt; add:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;print "finished page " + str(pages)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Our file should now look like this:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Load Libraries
from dpla.api import DPLA

# Set Variables
dpla = DPLA('Your-Key-Here')
# result = dpla.search('cooking', fields=['sourceResource'], page_size = 50)
all_records = []

# Define Functions
def pull_records(pages, end, size):
	while(pages &amp;lt;= end):
		paged_search = dpla.search('cooking',fields=['sourceResource'], page_size=size, page=pages)
		# print paged_search.items[2]
		save_each(paged_search)
		print "finished page " + str(pages)
		pages = pages + 1

def save_each(x):
	for each in x.items:
		all_records.append(each)

# Make Function Calls
# print result.items[1]
pull_records(2, 3, 50)
print all_records[30]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Let's test our function on a subset of the pages. Change &lt;span class="command"&gt;pull_records(2, 3, 50)&lt;/span&gt; to &lt;span class="command"&gt;pull_records(2, 5, 50)&lt;/span&gt; and change &lt;span class="command"&gt;print all_records[30]&lt;/span&gt; to &lt;span class="command"&gt;print all_records[150]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Save and run in Terminal.&lt;/p&gt;

&lt;h3 id="group-challenge"&gt;Group Challenge:&lt;/h3&gt;

&lt;p&gt;Go back to your pen and paper and diagram a &lt;code&gt;while&lt;/code&gt; loop. How does that diagram compare to the one you drew for the &lt;code&gt;for&lt;/code&gt; loop?&lt;/p&gt;

&lt;p&gt;&lt;span class="left"&gt;&lt;a href="module07.html"&gt;Previous Module&lt;/a&gt;&lt;/span&gt;
&lt;span class="right"&gt;&lt;a href="module09.html"&gt;Next Module&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Save Results to a File</title>
    <link rel="alternate" href="http://blog.url.com/modules/module13.html"/>
    <id>http://blog.url.com/modules/module13.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-31T22:07:14-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;We are almost there! We have been creating some interesting data on the word frequencies within the description fields. But so far, all of our results are stuck in Terminal, which makes it difficult for us to reuse them. So for this final module, we will write out the results of our count to a CSV (comma separated value) file.&lt;/p&gt;

&lt;h3 id="create-a-new-csv-file"&gt;Create a New CSV File&lt;/h3&gt;

&lt;p&gt;As we did when we wrote our JSON results, we will start by telling Python to open a CSV file and assign to a variable.&lt;/p&gt;

&lt;p&gt;Currently our &lt;code&gt;text_mining.py&lt;/code&gt; file should look like this:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.probability import *

# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Load in Stopwords Library
stopwords = stopwords.words('english')

word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
            word_set.append(word.lower())
    return word_set

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economics')
#print text.collocations()
#print text.similar('Pot')

normalize_text(text)

fd = FreqDist(word_set)
print fd.most_common(200)
print fd.hapaxes()
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;To create CSV files, we need to import the &lt;code&gt;csv&lt;/code&gt; library, which is preinstalled, but not preloaded in Python. To do that, add &lt;span class="command"&gt;import csv&lt;/span&gt; to our list of libraries at the top of the file.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.probability import *
import csv
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now, we can create our CSV file right after the line where we opened the JSON file. CSV files open a little differently than text files, in that we open the file with a "writer" helper.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

file = csv.writer(open('word_frequencies.csv', 'wb'))
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now, at the bottom of &lt;code&gt;text_mining.py&lt;/code&gt;, we can save the key (the word) and the count (the frequency) as two columns in the CSV file &lt;code&gt;word_frequencies.csv&lt;/code&gt;. If you did the graphing challenge, be sure to comment out &lt;span class="command"&gt;fd.plot(50,cumulative=False)&lt;/span&gt; as well. The plotting function and the write csv functions don't work well together.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;for key, count in fd.most_common(200):
    file.writerow([key, count])
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;The final product should look like this:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.probability import *
import csv

# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

file = csv.writer(open('word_frequencies.csv', 'w'))

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Load in Stopwords Library
stopwords = stopwords.words('english')

word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
            word_set.append(word.lower())
    return word_set

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economics')
#print text.collocations()
#print text.similar('Pot')

normalize_text(text)

fd = FreqDist(word_set)
print fd.most_common(200)
#print fd.hapaxes()
#fd.plot(50,cumulative=False)

# Print results to a CSV file
for key, count in fd.most_common(200):
    file.writerow([key, count])
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;You can open your CSV file using Terminal by typing or by looking within your "dhb_awesome" directory:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;open word_frequencies.csv
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;The file will most likely open in Excel or a similar program. &lt;/p&gt;

&lt;p&gt;Look over your results. What patterns strike you as interesting? As expected? As unexpected? What additional questions do these word frequencies raise? Now that you have this data, what additional information do you need to know to interpret the patterns we see here?&lt;/p&gt;

&lt;p&gt;&lt;span class="left"&gt;&lt;a href="module12.html"&gt;Previous Module&lt;/a&gt;&lt;/span&gt;
&lt;span class="right"&gt;&lt;a href="module14.html"&gt;Next Module&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Analyzing a Subset of the Data (Part 2)</title>
    <link rel="alternate" href="http://blog.url.com/modules/module12.html"/>
    <id>http://blog.url.com/modules/module12.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-31T15:11:21-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;In this module we will learn to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;normalize our text&lt;/li&gt;
  &lt;li&gt;use the Frequency Distribution library to find patterns in our text&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="normalizing-the-text"&gt;1. Normalizing the text&lt;/h3&gt;

&lt;p&gt;While we can get a lot of information from the entirety of the text, we can also find additional patterns once we do what is called "normalizing" the text. This entails removing all of the punctuation marks and transforming all the words to lower case. It also involves removing the small connection words such as "the" and "a", which are very common in a text, but carry less semantic meaning than nouns, verbs, and adjectives.&lt;/p&gt;

&lt;p&gt;To clean up the text, we need to work through each word and and run it through a series of checks or filters.&lt;/p&gt;

&lt;p&gt;Open your "&lt;code&gt;text_mining.py&lt;/code&gt;" file. First, comment out &lt;span class="command"&gt;print text.similar('Pot')&lt;/span&gt;. Next, let's create a new function that normalizes the text:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economics')
#print text.collocations()
#print text.similar('Pot')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;To work through the words, we can use a new "&lt;code&gt;for&lt;/code&gt; loop" and save all of the approved words into a new array named "&lt;code&gt;word_set&lt;/code&gt;". Create your new array at the end of the "Set Variables" section.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;First, for each word, let's check if it is alpha-numeric:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation
        if word.isalpha()
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Next, let's check the word against the NLTK collection of stopwords. First, let's load in the NLTK stopwords and assign them to the variable "&lt;code&gt;stopwords&lt;/code&gt;":&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize

# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Load Stopwords
stopwords = stopwords.words('english')

word_set = []
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Next, we need to transform each word to lower-case, because the stopwords list won't catch the uppercase words. We do this by adding &lt;span class="command"&gt;.lower()&lt;/span&gt; to word as we check the words against the "stopwords" collection.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Finally, we will add those words that pass through the filters to the "&lt;code&gt;word_set&lt;/code&gt;" list and tell the function to return the whole list once it is finished.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
            word_set.append(word.lower())
    return word_set
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;The last step is to call our "&lt;code&gt;normalize_text&lt;/code&gt;" function and pass in our text.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economy')
#print text.collocations()
#print text.similar('Pot')

normalize_text(text)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Well done! We're now ready to calculate word frequencies using the Frequency Distribution library.&lt;/p&gt;

&lt;h3 id="get-word-frequencies"&gt;2. Get Word Frequencies&lt;/h3&gt;

&lt;p&gt;Our Python file should now look like this:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize

# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Load in Stopwords Library
stopwords = stopwords.words('english')

word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
            word_set.append(word.lower())
    return word_set

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economy')
#print text.collocations()
#print text.similar('Pot')

normalize_text(text)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We need one more library to get word frequencies from our array of approved words. After &lt;span class="command"&gt;from nltk import word_tokenize&lt;/span&gt;, add:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;from nltk.probability import *
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now, we will use the Frequency Distribution method to get word counts. NLTK is a very powerful library, and by using it, we can draw on the work others to build and verify that the functions do what they say they do. There is no need to reinvent the wheel with every new script. Let's first use the library, and then work through what it did.&lt;/p&gt;

&lt;p&gt;The first step is to run the Frequency Distribution method and save the results to a variable so they are easier to use. After &lt;span class="command"&gt;normalize_text(text)&lt;/span&gt; add:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;fd = FreqDist(word_set)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;To see the 200 most common words, add to the bottom of the 'Make Function Calls' section:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;print fd.most_common(200)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;and run your script.&lt;/p&gt;

&lt;p&gt;Frequency Distribution goes through the words in our list, checks if it has seen that word before, if yes, adds 1 to the count for that word, and if not, notes it as a new word with a count of 1.&lt;/p&gt;

&lt;p&gt;Grab your drawing materials again, and draw a diagram of what the Frequency Distribution function is doing.&lt;/p&gt;

&lt;p&gt;Your file should look like:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.probability import *

# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Load in Stopwords Library
stopwords = stopwords.words('english')

word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
            word_set.append(word.lower())
    return word_set

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economy')
#print text.collocations()
#print text.similar('Pot')

normalize_text(text)

fd = FreqDist(word_set)
print fd.most_common(200)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;You can comment out print statements along the way if you are seeing too much information. &lt;/p&gt;

&lt;h3 id="bonus-challenge-for-the-ambitious"&gt;Bonus Challenge (for the ambitious)&lt;/h3&gt;

&lt;p&gt;For some super magic, you can also generate a plot of the most frequent words. You will need to install &lt;code&gt;numpy&lt;/code&gt; with &lt;span class="command"&gt;pip install -U numpy&lt;/span&gt; and &lt;code&gt;matplotlib&lt;/code&gt; with &lt;span class="command"&gt;pip install matplotlib&lt;/span&gt;. Remember, if you get an error, try running the command again with &lt;code&gt;sudo&lt;/code&gt; (e.g. &lt;code&gt;sudo pip install -U numpy&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Add to your script:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;fd.plot(50,cumulative=False)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;and run.&lt;/p&gt;

&lt;p&gt;&lt;span class="left"&gt;&lt;a href="module11.html"&gt;Previous Module&lt;/a&gt;&lt;/span&gt;
&lt;span class="right"&gt;&lt;a href="module13.html"&gt;Next Module&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Analyzing the Data (Part 1)</title>
    <link rel="alternate" href="http://blog.url.com/modules/module11.html"/>
    <id>http://blog.url.com/modules/module11.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-31T15:39:46-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;Up to this point, we have been focusing on molding the data we got back from the DPLA into different formats. First, we chose the part of the data we wanted to save locally and looped through all of the search result "pages" to get our large dataset. Then, because our research question involved language use and text, we transformed the data again into a format that suits the kind of analysis we want to do. Now, we get to take the results of our hard work and start to interrogate our data.&lt;/p&gt;

&lt;p&gt;In this module we will:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;install NLTK library&lt;/li&gt;
  &lt;li&gt;use NLTK to do see patterns in the text&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="installing-nltk"&gt;1. Installing NLTK&lt;/h3&gt;

&lt;p&gt;We are interested in the language used in the three fields we singled out across all the "cooking" items in the DPLA database. Fortunately, there is good support within Python for text analysis and one powerful library we can use is the Natural Language ToolKit (or NLTK).&lt;/p&gt;

&lt;p&gt;To install NLTK, let's go back to our Terminal and use &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Run &lt;span class="command"&gt;pip install nltk&lt;/span&gt;. You may need to use &lt;span class="command"&gt;sudo pip install nltk&lt;/span&gt;  (Mac).&lt;/p&gt;

&lt;p&gt;There are also a number of datasets available for use with NLTK. For our purposes, we will only be using the "stopwords" dataset. You can browse the list of all the datasets you could download and use at &lt;a href="http://www.nltk.org/nltk_data/"&gt;http://www.nltk.org/nltk_data/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To download the stopwords, we are going back into the Python Interactive Interpreter. Run &lt;span class="command"&gt;python&lt;/span&gt;. Your Terminal window should now look something like this:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;Python 2.7.5 (default, Mar  9 2014, 22:15:05)
[GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
&amp;gt;&amp;gt;&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Type &lt;span class="command"&gt;import nltk&lt;/span&gt; and press Enter.&lt;/p&gt;

&lt;p&gt;Next type &lt;span class="command"&gt;nltk.download('stopwords')&lt;/span&gt; and press Enter.&lt;/p&gt;

&lt;p&gt;Once you see&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;True
&amp;gt;&amp;gt;&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;you have successfully downloaded the stopwords file.&lt;/p&gt;

&lt;p&gt;You will also need to download a tokenizing library. Still in the Python interpretor, run&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;nltk.download('punkt')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Again, once you see&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;True
&amp;gt;&amp;gt;&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;the download is complete.&lt;/p&gt;

&lt;p&gt;You can now exit the Python Interactive Interpreter using &lt;span class="command"&gt;quit()&lt;/span&gt;&lt;/p&gt;

&lt;h3 id="lets-start-text-mining"&gt;Let's Start Text Mining&lt;/h3&gt;

&lt;p&gt;Let's create a third script file, "&lt;code&gt;text_mining.py&lt;/code&gt;":&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;touch text_mining.py
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;(Windows):&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;New-Item -ItemType file text_mining.py
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;And open that script file in your text editor:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;open text_mining.py
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;(Windows):&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;Start notepad++ text_mining.py
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Let's start by importing the NLTK library and the stopwords:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Import Libraries
import nltk
from nltk.corpus import stopwords
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Next let's load in our text file:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Let's add a "&lt;code&gt;print&lt;/code&gt;" command so we can display part of our text to make sure everything is loading correctly so far:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Import Libraries
import nltk
from nltk.corpus import stopwords

# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

# Define Functions

# Make Function Calls
print cooking_text[0:20]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Save and run this script in Terminal. Let's take a look at the results. One thing to note here is that Python treats the text as a list of letters.&lt;/p&gt;

&lt;p&gt;For our purposes, we want to work with the words, so let's use a function called "&lt;code&gt;tokenize&lt;/code&gt;" from the NLTK library. The NLTK "&lt;code&gt;tokenize&lt;/code&gt;" method has rules for splitting strings of text in to what we view as words. As the tokenizer goes through the text, it evaluates white space and punctuation, and then bundles the letters between the white space and punctuation together into "tokens". When we started, Python considered our text to be a list of letters; after tokenizing, the computer sees the text as a list of "words".&lt;/p&gt;

&lt;p&gt;After "&lt;code&gt;from nltk.corpus import stopwords&lt;/code&gt;" in the variables section, add:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;from nltk import word_tokenize
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now let's comment out the last print statement and transform our "words" into tokens using the "&lt;code&gt;word_tokenize&lt;/code&gt;" method. To see what has happened, let's also print the first &lt;strong&gt;10&lt;/strong&gt; tokens.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize

# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

cooking_tokens = word_tokenize(cooking_text)

# Define Functions

# Make Function Calls
#print cooking_text[0:20]
print cooking_tokens[0:10]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Next, we need to do one more transformation on our words so that they will play nicely with NLTK. Comment out "&lt;code&gt;print cooking_tokens[0:10]&lt;/code&gt;" and add to the variables section after &lt;span class="command"&gt;cooking_tokens…&lt;/span&gt;:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;text = nltk.Text(cooking_tokens)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Your file should now look like this:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize

# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Define Functions

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;The first thing we can do to get a sense of the words in our dataset is to use the "&lt;code&gt;concordance&lt;/code&gt;" method within NLTK. This will print all the instances of a word with the surrounding words for context.&lt;/p&gt;

&lt;p&gt;After &lt;span class="command"&gt;#print cooking_tokens[0:10]&lt;/span&gt;, add:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;print text.concordance('cooking')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Save and run your script.&lt;/p&gt;

&lt;p&gt;Pretty cool! Now change "cooking" to "economics", save and run the script, and see what the output is:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;print text.concordance('economics')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Try some other words to get a sense of the word usage in the "&lt;code&gt;text_results&lt;/code&gt;" file. You can either replace the word or add additional &lt;code&gt;print&lt;/code&gt; statements.&lt;/p&gt;

&lt;p&gt;Another useful method is "&lt;code&gt;collocation&lt;/code&gt;". This shows us all the words that tend to appear together throughout the corpus.&lt;/p&gt;

&lt;p&gt;Comment out &lt;span class="command"&gt;print text.concordance('your_last_word')&lt;/span&gt; and add &lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;print text.collocations()
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Save and run your script.&lt;/p&gt;

&lt;p&gt;One more method that is useful for surveying our data is "&lt;code&gt;similar&lt;/code&gt;". This shows us words that are used similarly to the word we give it.&lt;/p&gt;

&lt;p&gt;Comment out &lt;span class="command"&gt; print text.collocations()&lt;/span&gt; and add:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;print text.similar('Pot')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Your script should now look like:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize

# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Define Functions

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economics')
#print text.collocations()

print text.similar('Pot')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;What other patterns might be interesting to know about the words used to describe objects related to "cooking"?&lt;/p&gt;

&lt;p&gt;In the next module, we will look at word counts to find the most common words used across all of the different DPLA contributors.&lt;/p&gt;

&lt;p&gt;&lt;span class="left"&gt;&lt;a href="module10.html"&gt;Previous Module&lt;/a&gt;&lt;/span&gt;
&lt;span class="right"&gt;&lt;a href="module12.html"&gt;Next Module&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Working with Local Data</title>
    <link rel="alternate" href="http://blog.url.com/modules/module10.html"/>
    <id>http://blog.url.com/modules/module10.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-31T15:21:52-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;In this module we will learn to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;create a new script file and load in our JSON data&lt;/li&gt;
  &lt;li&gt;select data from particular fields and deal with missing data&lt;/li&gt;
  &lt;li&gt;convert our JSON data to strings of text&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that we have a very large file of JSON data, we can work locally to find patterns that we could not find using the online interface for the DPLA's holdings. Since we are interested in the language being used in order to investigate how the descriptions of "cooking" items are gendered, our next step is to clean up that data and save it as text only so we can analyize it in later modules.&lt;/p&gt;

&lt;h3 id="create-a-new-script-and-load-in-our-data"&gt;1. Create a New Script and Load in Our Data&lt;/h3&gt;

&lt;p&gt;It is time to create a new script file!&lt;/p&gt;

&lt;p&gt;Go to Terminal and type:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;touch my_second_script.py
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;or on Windows:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;New-Item -ItemType file my_second_script.py
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;To open your new script file, type:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;open my_second_script.py
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;or on Windows:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;Start notepad++ my_second_script.py
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;First, we will need the JSON library again. To add this, add &lt;span class="command"&gt;import json&lt;/span&gt; to the very top of the file.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Load Libraries
import json
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;The next thing is to load up the data from our "&lt;code&gt;search_results.json&lt;/code&gt;" file.&lt;/p&gt;

&lt;p&gt;The structure for this is:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Set Variables
with open("search_results.json") as json_file:
    json_data = json.load(json_file)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Here, we tell Python to open our "&lt;code&gt;search_results.json&lt;/code&gt;" file and assign it to the variable "&lt;code&gt;json_file&lt;/code&gt;". Then we use the "&lt;code&gt;load&lt;/code&gt;" method from the JSON library to load up the data and save it as the variable "&lt;code&gt;json_data&lt;/code&gt;".&lt;/p&gt;

&lt;p&gt;Our second script file should now look like:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Load Libraries
import json

# Set Variables
with open("search_results.json") as json_file:
    json_data = json.load(json_file)

# Define Functions

# Make Function Calls
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;To make sure this worked, let's print out one item from the JSON data. Work with your table to add a "&lt;code&gt;print&lt;/code&gt;" statement that prints the second item in the "&lt;code&gt;json_data&lt;/code&gt;" list.&lt;/p&gt;

&lt;h3 id="select-the-relevant-text-data"&gt;2. Select the Relevant Text Data&lt;/h3&gt;

&lt;p&gt;Our research question for this data is how, across the entire DPLA collection, the descriptions of items related to cooking are gendered. To pursue this question, we need to do some basic text analysis, so let's save our search results in a format that makes it easy to do that.&lt;/p&gt;

&lt;p&gt;The next step is to select the fields that will be most helpful for analyzing how "cooking" items are described across the items in the DPLA.&lt;/p&gt;

&lt;p&gt;Looking at our items, there are three main fields containing description-type information for our different "cooking" items: the title, the description, and the subject headings.&lt;/p&gt;

&lt;p&gt;To start, let's make a new function called "&lt;code&gt;get_text&lt;/code&gt;" that takes in our "&lt;code&gt;json_data&lt;/code&gt;" list:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Define Functions
def get_text(json_data):
    # Do something to each item in json_data
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Next, for each item in that list, we want to look for the "title", the "description", and the "subject heading fields". This means using another "&lt;code&gt;for&lt;/code&gt; loop".&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Define Functions
def get_text(json_data):
    for each in json_data:
        # Get the titles
        # Get the descriptions
        # Get the subject headings
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We can get the title, description, and subject headings by looking for those "keys" within each item in our "&lt;code&gt;json_data&lt;/code&gt;" list.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Define Functions
def get_text(json_data):
    for each in json_data:
        # Get the title
        find_titles = each['sourceResource']['title']

        # Get the Description
        find_descriptions = each['sourceResource']['description']

        # Get the Subject Headings
        find_subjects = each['sourceResource']['subject']
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;So far so good. But what if one of these fields is missing? Programming languages are very literal: if you tell it to do something that it cannot do, it just stops and gives you an error. To see this in action, let's add a line in "Make Function Calls" to call the function.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Load Libraries
import json

# Set Variables
with open('search_results.json') as json_file:
    json_data = json.load(json_file)

# Define Functions
def get_text(json_data):
    for each in json_data:
        # Get the Titles
        find_titles = each['sourceResource']['title']

        # Get the Descriptions
        find_descriptions = each['sourceResource']['description']

        # Get the Subject Headings
        find_subjects = each['sourceResource']['subject']

# Make Function Calls
print json_data[1]
get_text(json_data)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Towards the end of the error message, you should see a line that says "&lt;code&gt;KeyError: 'sourceResource.description'&lt;/code&gt;". This is Python telling you that it cannot find a key "description" in one of the resources.&lt;/p&gt;

&lt;p&gt;To deal with this, we use "&lt;code&gt;try&lt;/code&gt;" and "&lt;code&gt;except&lt;/code&gt;" – we will tell the computer to try to find the keys, but if it doesn't, to assign the value to an empty space and move on.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Define Functions
def get_text(json_data):
    for each in json_data:
        # Get the Titles
        try:
            find_titles = each['sourceResource']['title']
        except:
            title = ' '

        # Get the Descriptions
        try:
            find_descriptions = each['sourceResource']['description']
        except:
            description = ' '

        # Get the Subject Headings
        try:
            find_subjects = each['sourceResource']['subject']
        except:
            subject = ' '
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;There are two more steps we need to get the text ready to save. The first step, using "&lt;code&gt;else&lt;/code&gt;", is to get all of the data out of lists and make sure that it is encoded so that Python knows which letters correspond to the data bits.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Define Functions
def get_text(json_data):
    for each in json_data:
        # Get the Titles
        try:
            find_titles = each['sourceResource']['title']
            if isinstance(find_titles, basestring):
                title = find_titles.encode('utf8')
            else:
                for each in find_titles:
                    title = each.encode('utf8')
        except:
            title = ' '

        # Get the Descriptions
        try:
            find_descriptions = each['sourceResource']['description']
            if isinstance(find_descriptions, basestring):
                description = find_descriptions.encode('utf8')
            else:
                for each in find_descriptions:
                    description = each.encode('utf8')
        except:
            description = ' '

        # Get the Subject Headings
        try:
            find_subjects = each['sourceResource']['subject']
            if isinstance(find_subjects, basestring):
                subject = find_subjects.encode('utf8')
            else:
                subject_list = []
                for each in find_subjects:
                    subject_list.append(each['name'])

                combined_subjects = ', '.join(subject_list)
                subject = combined_subjects.encode('utf8')
        except:
            subject = ' '
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;You might notice two different things with this function. First, we are reusing variable names. Variables hold the last thing passed to them, so we can overwrite the value of a variable to update it as we go along.&lt;/p&gt;

&lt;p&gt;Second, we are handling the subject field differently than the title and description fields. This is because the subject key holds an additional list, rather than just values.&lt;/p&gt;

&lt;p&gt;Work with your group to map out what is going on in the subject section of the function.&lt;/p&gt;

&lt;p&gt;The last is to save this data into a text file. Similar to last time, we need to set up the file that will receive that data:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Load Libraries
import json

# Set Variables
with open("search_results.json") as json_file:
    json_data = json.load(json_file)

f = open('text_results.txt', 'w')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now, let's go back to the function and put all our pieces together into a new variable.&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Define Functions
def get_text(json_data):
    for each in json_data:
        # Get the Titles
        try:
            find_titles = each['sourceResource']['title']
            if isinstance(find_titles, basestring):
                title = find_titles.encode('utf8')
            else:
                for each in find_titles:
                    title = each.encode('utf8')
        except:
            title = ' '

        # Get the Descriptions
        try:
            find_descriptions = each['sourceResource']['description']
            if isinstance(find_descriptions, basestring):
                description = find_descriptions.encode('utf8')
            else:
                for each in find_descriptions:
                    description = each.encode('utf8')
        except:
            description = ' '

        # Get the Subject Headings
        try:
            find_subjects = each['sourceResource']['subject']
            if isinstance(find_subjects, basestring):
                subject = find_subjects.encode('utf8')
            else:
                subject_list = []
                for each in find_subjects:
                    subject_list.append(each['name'])

                combined_subjects = ', '.join(subject_list)
                subject = combined_subjects.encode('utf8')
        except:
            subject = ' '

        # Combine the data into a single variable in the form of a sentence
        data = title + '; ' + description + '; ' + subject + '. \n'
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;This line takes the information held in the title field, adds a ";" and a space, takes the information in the description field, add '; and a space, and finally, takes the list held in the subject field and smashes it together into a string, separated by a comma and a space. Finally, the "\n" adds an "Enter" to the end of the line, so that the information for each item appears on a new line.&lt;/p&gt;

&lt;p&gt;The last step is to write all the information within "data" to our file directly under the line just added:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;        # Combine the data into a single variable in the form of a sentence
        data = title + '; ' + description + '; ' + subject + '. \n'

        # Write the sentence to the 'text_results' file
        f.write(data)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Last, let's add a call to the function:&lt;/p&gt;

&lt;div class="highlight plaintext"&gt;&lt;table style="border-spacing: 0"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="gutter gl" style="text-align: right"&gt;&lt;pre class="lineno"&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;# Load Libraries
import json

# Set Variables
with open("search_results.json") as json_file:
    json_data = json.load(json_file)

f = open('text_results.txt', 'w')

# Define Functions
def get_text(json_data):
    for each in json_data:
        # Get the Titles
        try:
            find_titles = each['sourceResource']['title']
            if isinstance(find_titles, basestring):
                title = find_titles.encode('utf8')
            else:
                for each in find_titles:
                    title = each.encode('utf8')
        except:
            title = ' '

        # Get the Descriptions
        try:
            find_descriptions = each['sourceResource']['description']
            if isinstance(find_descriptions, basestring):
                description = find_descriptions.encode('utf8')
            else:
                for each in find_descriptions:
                    description = each.encode('utf8')
        except:
            description = ' '

        # Get the Subject Headings
        try:
            find_subjects = each['sourceResource']['subject']
            if isinstance(find_subjects, basestring):
                subject = find_subjects.encode('utf8')
            else:
                subject_list = []
                for each in find_subjects:
                    subject_list.append(each['name'])

                combined_subjects = ', '.join(subject_list)
                subject = combined_subjects.encode('utf8')
        except:
            subject = ' '

        # Combine the data into a single variable in the form of a sentence
        data = title + '; ' + description + '; ' + subject + '. \n'

        # Write the sentence to the 'text_results' file
        f.write(data)

# Make Function Calls
get_text(json_data)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Save and run your second Python script. You should now have a file named "&lt;code&gt;text_results.txt&lt;/code&gt;" in your "&lt;code&gt;dhb_awesome&lt;/code&gt;" directory. If you open that file, you should see lines of beautiful text ready for analysis.&lt;/p&gt;

&lt;p&gt;In the next module, we will use the Natural Language ToolKit, a powerful Python library for working with text, to find patterns in the text data.&lt;/p&gt;

&lt;p&gt;&lt;span class="left"&gt;&lt;a href="module09.html"&gt;Previous Module&lt;/a&gt;&lt;/span&gt;
&lt;span class="right"&gt;&lt;a href="module11.html"&gt;Next Module&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
</content>
  </entry>
</feed>
