<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog Name</title>
  <subtitle>Blog subtitle</subtitle>
  <id>http://blog.url.com/modules</id>
  <link href="http://blog.url.com/modules"/>
  <link href="http://blog.url.com/feed.xml" rel="self"/>
  <updated>2014-09-18T20:00:00-04:00</updated>
  <author>
    <name>Blog Author</name>
  </author>
  <entry>
    <title>Save Results to File</title>
    <link rel="alternate" href="http://blog.url.com/modules/module13-save.html"/>
    <id>http://blog.url.com/modules/module13-save.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-12T11:34:22-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;We are almost there! We have been creating some interesting data on the word frequencies within the description fields. But so far, all of our results are stuck in terminal, which makes it difficult for us to reuse them. So for this final module, we will write out the results of our count to a CSV (comma separated value) file.&lt;/p&gt;

&lt;h3 id="create-a-new-csv-file"&gt;Create a New CSV File&lt;/h3&gt;

&lt;p&gt;As we did when we wrote our json results, we will start by telling Python to open a CSV file and assign to a variable.&lt;/p&gt;

&lt;p&gt;Currently our script should look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import json
from collections import Counter
import nltk
from nltk.corpus import stopwords

with open("search_results.json") as json_file:
    json_data = json.load(json_file)
    
#print json_data[1]
#print json.dumps(json_data[1], sort_keys=True, indent=4, separators=(',', ': '))
#print json_data[1]['sourceResource']['description']

description_words = []
stop = stopwords.words('english')

def remove_stops_and_add(word):
    if word.lower() not in stop:
        if not word.isdigit():
            if word.isalnum():
                description_words.append(word.lower())
            
def get_words():
    for each in json_data:
        try:
            descriptions = each['sourceResource']['description']
            if isinstance(descriptions, basestring):
                words = descriptions.split()
            else:
                for line in descriptions:
                    words = line.split()
            #print words
            
            for word in words:
                remove_stops_and_add(word)
            
        except KeyError:
            print "Description Missing"

get_words()

#print description_words[999] 

c = Counter(description_words)

print c.most_common(200)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create CSV files, we need to import the csv library, which is preinstalled, but not preloaded in python. To do that, add &lt;span class="command"&gt;import csv&lt;/span&gt; to our list of libraries at the top of the file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import json
from collections import Counter
import nltk
from nltk.corpus import stopwords
import csv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we can create our CSV file right after the line where we opened the json file. CSV files open a little differently than text files, in that we open the file with a "writer" helper.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;with open("search_results.json") as json_file:
    json_data = json.load(json_file)

file = csv.writer(open("word_frequencies.csv", "w"))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, at the end of our file, we can save the key(the word) and the count(the frequency) as two columns in our csv file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;get_words()

#print description_words[999] 

c = Counter(description_words)

print c.most_common(200)

for key, count in c.most_common(200):
    file.writerow([key, count])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can now open your csv file using terminal by typing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;open word_frequencies.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will most likely open in excel or a similar program. &lt;/p&gt;

&lt;p&gt;Look over your results. What patterns strike you as interesting? As expected? As unexpected? What additional questions do these word frequencies raise? Now that you have this data, what additional information do you need to know to interpret the patters we see here?&lt;/p&gt;

&lt;h3 id="what-we-learned"&gt;What We Learned&lt;/h3&gt;

&lt;p&gt;In this module, we learned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;to use the CSV library to save our results in csv format&lt;/li&gt;
&lt;/ul&gt;

</content>
  </entry>
  <entry>
    <title>Analyzing a Subset of the Data</title>
    <link rel="alternate" href="http://blog.url.com/modules/module12-analyze.html"/>
    <id>http://blog.url.com/modules/module12-analyze.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-12T11:23:28-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;We now have a bag of words that we can do some analysis on. In this module, we will ask some basic questions about word frequencies, make some adjustments to our collection of words to improve the results, and discuss what additional questions those patterns might raise.&lt;/p&gt;

&lt;h3 id="getting-word-frequencies"&gt;Getting Word Frequencies&lt;/h3&gt;

&lt;p&gt;To get word frequencies, we will use the "Counter" library we loaded earlier. Looking at the &lt;a href="https://docs.python.org/2/library/collections.html#collections.Counter"&gt;documentation for the library&lt;/a&gt;, we see that if we pass "Counter" a list, we can get back information regarding the items in the list, including the most common items.&lt;/p&gt;

&lt;p&gt;Add a new line to the bottom of "my_second_script.py" as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c = Counter(description_words)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the capitalization here. Because the library has the name of "Counter", we need to keep that "C" uppercase whenever we use the library. &lt;/p&gt;

&lt;p&gt;Now we will use the method "most_common" and let's start with the top 25 words:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print c.most_common(25)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Comment out the "print words" statements from the last module (so that we can see what is going on), then save and run the file.&lt;/p&gt;

&lt;p&gt;You should see a list of words and frequencies that looks like this: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(u'the', 11873), (u'of', 8435), (u'and', 7689), (u'in', 5553), (u'a', 4542), (u'to', 4375), (u'is', 2150), (u'1', 2090), (u'for', 2080), (u'on', 1897), (u'The', 1808), (u'was', 1632), (u'with', 1590), (u';', 1440), (u'by', 1408), (u':', 1389), (u'photograph', 1324), (u'from', 1191), (u'at', 1179), (u'as', 1140), (u'that', 1136), (u'are', 1035), (u'x', 1023), (u'be', 990), (u'or', 979), (u'2', 877), (u'0', 776), (u'2014', 740), (u'I', 734), (u'were', 714), (u'b&amp;amp;w', 691), (u'cooking', 685), (u'3', 685), (u'in.', 675), (u'4', 674), (u'it', 665), (u'this', 663), (u'which', 632), (u'5', 578), (u'have', 571), (u'.', 543), (u'cm.', 513), (u'an', 501), (u'their', 494), (u'about', 489), (u'This', 488), (u'not', 477), (u'col.', 468), (u'negative,', 460), (u'It', 441), (u'these', 438), (u'12', 429), (u'A', 425), (u'will', 423), (u'35', 415), (u'has', 413), (u'In', 411), (u'mm.', 395), (u'Jun', 391), (u'p.', 386), (u'they', 384), (u'food', 381), (u'had', 379), (u'all', 376), (u'we', 368), (u'he', 367)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But for the most part, these words are not terribly interesting to us. The prevalence of "the" and "of" doesn't tell us much about how "cooking" is described and are making it hard to a pattern in the nouns and verbs. &lt;/p&gt;

&lt;p&gt;There is a second problem. If you notice, "the" is in the list twice, once as "the" and once as "The". Computers, being ever the literal processors, take the difference in case to indicate a difference in word. This can be very helpful in some circumstances, like finding names. But when just looking for a rough count of words, it creates noise.&lt;/p&gt;

&lt;p&gt;To address these problems, we can remove those function words and transform all the words to lowercase, using the Natural Language ToolKit.&lt;/p&gt;

&lt;h3 id="removing-stopwords"&gt;Removing StopWords&lt;/h3&gt;

&lt;p&gt;Going back to our "get_words" function, one way to remove the stopwords is to check if the word is in a standard list of function words before adding it to the bag.&lt;/p&gt;

&lt;p&gt;The logic here is, if the word is not a stopword, then we want to save it to our "description_words" list.&lt;/p&gt;

&lt;p&gt;To do this, let's create a second function for checking the stopwords.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def remove_stops_and_add(word):
    if word.lower() not in stop:
        description_words.append(word.lower())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we are combining two steps in one. We transform each word as it comes in to the lower case with ".lower()" before we check it against the stop words. Then we transform it again before we append it to our "description_words" list. &lt;/p&gt;

&lt;p&gt;We could also transform it once at the beginning of this function. Work with your table to work out how would you change the function to do that.&lt;/p&gt;

&lt;p&gt;Now, we also need to load up the stopwords from nltk library into a variable "stop". &lt;/p&gt;

&lt;p&gt;On the line after we set up our "description_words" list, add:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stop = stopwords.words('english')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we are loading up the collection of english stop words from the stopwords library.&lt;/p&gt;

&lt;p&gt;Finally, we need to call this new function inside our "get_words()" function. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
    try:
      descriptions = each['sourceResource']['description']
      if isinstance(descriptions, basestring):
        words = descriptions.split()
      else:
        for line in descriptions:
          words = line.split()
        #print words
        
      for word in words:
        remove_stops_and_add(word)

    except KeyError:
      print "Description Missing"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save and run your script.&lt;/p&gt;

&lt;p&gt;Now our results should look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[(u'1', 2090), (u'photograph', 1520), (u';', 1440), (u':', 1389), (u'x', 1057), (u'2', 877), (u'cooking', 779), (u'0', 776), (u'2014', 740), (u'b&amp;amp;w', 691), (u'3', 685), (u'in.', 675), (u'4', 674), (u'5', 578), (u'.', 543), (u'cm.', 513), (u'food', 471), (u'col.', 468), (u'may', 462), (u'negative,', 460), (u'one', 440), (u'12', 429), (u'35', 415), (u'p.', 408), (u'also', 404), (u'mm.', 396), (u'two', 393), (u'jun', 391), (u'6', 358), (u'would', 347), (u'clark', 345), (u'includes', 332), (u'time', 327), (u'molasses', 325), (u'new', 324), (u'people', 315), (u'20', 306), (u'well', 301), (u'state', 301), (u'early', 288), (u'work', 284), (u'water', 283), (u'university', 282), (u'positive,', 280), (u'home', 276), (u'joe', 275), (u'per', 270), (u'farm', 269), (u'information', 263), (u'school', 251), (u'three', 251), (u'-', 250), (u'first', 243), (u'8', 238), (u'used', 237), (u'7', 235), (u'good', 231), (u'10', 227), (u'large', 217), (u'no.', 216), (u'published', 213), (u'library', 207), (u'9', 206)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is much better! But now we have some distracting numbers and punctuation marks. So let's get rid of those too.&lt;/p&gt;

&lt;p&gt;Going back to our new "remove_stops_and_add()" function, let's add a few more filters that the words must pass through before we add it to the bag.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def remove_stops_and_add(word):
    if word.lower() not in stop:
        if not word.isdigit():
            description_words.append(word.lower())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This checks to see if the words is a digit and if it is not, then it allows it to pass. We also had some punctuation marks and abbreviations. To remove these, we can allow only those words that are alphanumeric to pass before we add to "description_words". &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def remove_stops_and_add(word):
    if word.lower() not in stop:
        if not word.isdigit():
            if word.isalnum():
                description_words.append(word.lower())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And well done! We now have a very interesting list of description words and their frequencies across all of the "cooking" items in the DPLA's holdings.&lt;/p&gt;

&lt;h3 id="bonus-challege"&gt;Bonus Challege&lt;/h3&gt;

&lt;p&gt;You can also use "stemming" to combine multipe forms of the same word, such as "photograph" and "photographs". Can you use the &lt;a href="http://www.nltk.org/api/nltk.stem.html"&gt;NLTK documentation&lt;/a&gt; to add another filter that stems the words before adding them to "description_words"?&lt;/p&gt;

&lt;h3 id="what-we-learned"&gt;What We Learned&lt;/h3&gt;

&lt;p&gt;In this module, we learned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;to use stopwords and filters to clean up noisy data&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry>
    <title>Gathering a Subset of the Data</title>
    <link rel="alternate" href="http://blog.url.com/modules/module11-subset.html"/>
    <id>http://blog.url.com/modules/module11-subset.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-12T09:49:08-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;In this module, we will learn how to collect and store the description data from each item in our "json_data" variable. To do this, we will first identify the location of the description with the data structure and then loop through each item, saving each of the words in the description into a new list.&lt;/p&gt;

&lt;h3 id="identifying-our-target-fields"&gt;Identifying Our Target Fields&lt;/h3&gt;

&lt;p&gt;Remember back to JSON and how it organizes data using key:value pairs? One of the most powerful features of JSON is that we are able to nest features and create lists within key:value pairs. This is useful for creating complex data structures. It also means that we have work within the hierarchy of key:value pairs to isolate particular values.&lt;/p&gt;

&lt;p&gt;In order to better see that hierarchy, let's &lt;strong&gt;Pretty Print&lt;/strong&gt; the JSON results, or print with the indentations and hierarchies visually displayed. &lt;/p&gt;

&lt;p&gt;Open "my_second_script.py" and &lt;strong&gt;comment out&lt;/strong&gt; "print jason_data[0]" by putting # at the beginning of the line. When your computer executes the file, it will skip all lines that start with a pound sign. This allows you to leave comments for yourself or to test new ways of doing things without loosing your work.&lt;/p&gt;

&lt;p&gt;At the bottom of your script, add the line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print json.dumps(json_data[1], sort_keys=True, indent=4, separators=(',', ': '))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save and run in Terminal. &lt;/p&gt;

&lt;p&gt;Work with your table to map out what this command did.&lt;/p&gt;

&lt;p&gt;We now need to find the "description" field within the json object. You can either read through all of the lines of code or you can search within Terminal by pressing "command F". &lt;/p&gt;

&lt;p&gt;Looking at the "description" field, work with your table to identify the field name it is nested under. &lt;/p&gt;

&lt;p&gt;The syntax for calling items within a data structure uses brackets to designate the key names or the position in the list of the item we want to interact with. We already use this syntax when we call the first item in the data by using &lt;span class="command"&gt;json_data[1]&lt;/span&gt;. This form means we want the first item in the json_data object. &lt;/p&gt;

&lt;p&gt;If we want the value associated with the "description" key, we use a similar format. &lt;/p&gt;

&lt;p&gt;To print only the description for the first item in our list, add the following to the bottom of your "my_second_script.py" file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print json_data[1]['sourceResource']['description']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Work with your table to print only the titles. What about only the subject headings?&lt;/p&gt;

&lt;h3 id="saving-the-descriptions"&gt;Saving the Descriptions&lt;/h3&gt;

&lt;p&gt;One way to look at the descriptions is to consider them as a collection, or list, of words. If we take all of the collections as a whole, we can ask if there are any noticable patterns in the words being used to describe these items related to cooking.&lt;/p&gt;

&lt;p&gt;One common way of dealing with text computationally is to translate the text into a "bag of words". In a bag of words approach, the order does not matter. What we are looking at instead is the collection of words and their frequencies.&lt;/p&gt;

&lt;p&gt;To create a "bag of words" from the descriptions we need to 1) create a list to store the words and 2) write a loop that takes every description and adds the words to the list.&lt;/p&gt;

&lt;p&gt;See if you can use the information in module 7 (functions and loops) to create an empty list.&lt;/p&gt;

&lt;p&gt;Currently our "my_second_script.py" file should look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import json
from collections import Counter
import nltk
from nltk.corpus import stopwords

with open("search_results.json") as json_file:
  json_data = json.load(json_file)
	
#print json_data[1]
#print json.dumps(json_data[1], sort_keys=True, indent=4, separators=(',', ': '))

print json_data[1]['sourceResource']['description']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I will use "description_words" as the variable for my empty list of stopwords. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;description_words = []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we will create a new function called "get_words"&lt;/p&gt;

&lt;p&gt;Declare the function by adding:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, tab in once and add a for-loop to move through each item in the json_data list:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, let's set up a variable to grab the descriptions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
    descriptions = each['sourceResource']['description']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, description holds the full sentence as a single object and we want to get the individual words. We can use a built in function called "split" that divides the sentence into an array of individual objects, splitting on the spaces and punctuation marks.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
    descriptions = each['sourceResource']['description']
    for line in descriptions:
      words = line.split()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To see what just happened, let's add a print statement and call the function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
    descriptions = each['sourceResource']['description']
    for line in descriptions:
      words = line.split()
      print words

get_words()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your file should now look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import json
from collections import Counter
import nltk
from nltk.corpus import stopwords

with open("search_results.json") as json_file:
  json_data = json.load(json_file)
	
#print json_data[1]
#print json.dumps(json_data[1], sort_keys=True, indent=4, separators=(',', ': '))

print json_data[1]['sourceResource']['description']

description_words = []

def get_words():
  for each in json_data:
    descriptions = each['sourceResource']['description']
    for line in descriptions:
      words = line.split()
      print words

get_words()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save the file and run it in the terminal.&lt;/p&gt;

&lt;p&gt;You might first notice that this script threw an error, saying:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KeyError: 'description'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One of the most important things in learning to work with code is learning to decipher error messages. If you read a little more in your terminal window, you'll see that the full message is something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jeriwieringa$ python my_second_script.py
Traceback (most recent call last):
  	File "my_second_script.py", line 21, in &amp;lt;module&amp;gt;
	get_words()
  	File "my_second_script.py", line 16, in get_words
	descriptions = each['sourceResource']['description']
KeyError: 'description'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have a traceback saying in our "get_words()" function there was an error on line 16 where we told it to save the value associated with the "description" key to the variable "descriptions". &lt;/p&gt;

&lt;p&gt;Let's see what is going. Comment out the call to "get_words" function.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# get_words()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let's print the first object in the json_data list. Uncomment our print statement at the beginning of the file that pretty-prints the json. Change the 1 to 0, the save and run the file.&lt;/p&gt;

&lt;p&gt;Search for 'description' under 'sourceResource'. If you can't find it, it's because this item appears to be missing "description" information. Because the computer can't find the description, it returns an error and stops.&lt;/p&gt;

&lt;p&gt;Getting around this problem requires what is called "handling exceptions". We had expected a particular pattern in our data (that there was "description" keys nested under "sourceResource" keys) but we've encountered an exception to that general pattern. &lt;/p&gt;

&lt;p&gt;To tell the computer to keep moving if it encounters an item it cannot parse because of a KeyError, we will adjust our get_words() function as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
    try:
      descriptions = each['sourceResource']['description']
      for line in descriptions:
      words = line.split()
        print words
    except KeyError:
      print "Description Missing"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Un-comment "get_words()", save, and run the file.&lt;/p&gt;

&lt;p&gt;Our script should no longer error.&lt;/p&gt;

&lt;p&gt;But, now it seems that sometime we are getting a list of letters, and sometimes a list of words. So one more adjustment is in order. We need to perform a check, where if the value of descriptions is a string, the function does one thing, and if it is not a string, it does something else. Our current code works well on the lists, so we will keep that for not strings.&lt;/p&gt;

&lt;p&gt;To check if the value in "descriptions" in a string, we can add the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
  for each in json_data:
    try:
    descriptions = each['sourceResource']['description']
    if isinstance(descriptions, basestring):
      words = descriptions.split()
    else:
      for line in descriptions:
        words = line.split()
        print words
  except KeyError:
    print "Description Missing"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looking through the information printing to our terminal window, it looks like we have correctly split out all of the words.&lt;/p&gt;

&lt;h3 id="adding-the-words-to-our-bag-of-words"&gt;Adding the words to our "bag of words"&lt;/h3&gt;

&lt;p&gt;The final step in creating our subset of data is to add the words into our "description_words" list.&lt;/p&gt;

&lt;p&gt;To do this, we will need to loop through the "words" list and add each word to the "description_words" list. This means another for-loop within the "try" second of our function.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_words():
for each in json_data:
  try:
    descriptions = each['sourceResource']['description']
    if isinstance(descriptions, basestring):
      words = descriptions.split()
    else:
      for line in descriptions:
        words = line.split()
    print words

    for word in words:
      description_words.append(word)

  except KeyError:
    print "Description Missing"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now to check that everything worked ok, let's run the function and print out the 1000 word in our "descriptions_words"&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import json
from collections import Counter
import nltk
from nltk.corpus import stopwords

with open("search_results.json") as json_file:
json_data = json.load(json_file)
	
#print json_data[1]
#print json.dumps(json_data[0], sort_keys=True, indent=4, separators=(',', ': '))

#print json_data[0]['sourceResource']['description']

description_words = []

def get_words():
for each in json_data:
  try:
    descriptions = each['sourceResource']['description']
    if isinstance(descriptions, basestring):
      words = descriptions.split()
    else:
      for line in descriptions:
        words = line.split()
    print words

    for word in words:
      description_words.append(word)

  except KeyError:
    print "Description Missing"
  
get_words()
print description_words[999]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id="bonus-challenges"&gt;Bonus Challenges:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Can you modify the function to also add the words of the title to our bag of words?&lt;/li&gt;
  &lt;li&gt;Can you also add the subject headings?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="what-we-learned"&gt;What We Learned&lt;/h3&gt;

&lt;p&gt;In this module, we learned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;to pretty-print json with Python&lt;/li&gt;
  &lt;li&gt;to identify the nested key:value pairs we want to analyze&lt;/li&gt;
  &lt;li&gt;to split a list &lt;/li&gt;
  &lt;li&gt;to trouble shoot from an error report&lt;/li&gt;
  &lt;li&gt;to handle exceptions&lt;/li&gt;
  &lt;li&gt;to check type and use an "if/else" statement&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry>
    <title>Working with Local Data</title>
    <link rel="alternate" href="http://blog.url.com/modules/module10-local_data.html"/>
    <id>http://blog.url.com/modules/module10-local_data.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-12T11:37:26-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;Now that we have a very large file of JSON data, we can work locally to find  patterns that we could not find using the online interface for the DPLA's holdings. &lt;/p&gt;

&lt;p&gt;In this module, we will create a new script and learn how to load data from our "search_results.json" file. We will also install the Natural Language ToolKit library in order to do some text analysis on our data.&lt;/p&gt;

&lt;h3 id="loading-from-a-local-file"&gt;Loading from a Local File&lt;/h3&gt;

&lt;p&gt;It is time to create a new script file!&lt;/p&gt;

&lt;p&gt;Go to terminal and type:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;touch my_second_script.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To open your new script file, type:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;open my_second_script.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To begin, let's load in some of the libraries we will need for this next phase of the workshop.&lt;/p&gt;

&lt;p&gt;First, we will need the json library again. To add this, type &lt;span class="command"&gt;import json&lt;/span&gt; at the very top of the file.&lt;/p&gt;

&lt;p&gt;We are also going to use a library called "collections" and a specific part of that library called "Counter". To import this, type &lt;span class="command"&gt;from collections import Counter&lt;/span&gt; on the next line.&lt;/p&gt;

&lt;p&gt;The next thing is to load up the data from our "search_results.json" file.&lt;/p&gt;

&lt;p&gt;The structure for this is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;with open("search_results.json") as json_file:
	json_data = json.load(json_file)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, we tell Python to "open" our "search_results.json" file and assign it to the variable "json_file". The we use the "load" method in the "json.library" to load up the data and save it as the variable "json_data". &lt;/p&gt;

&lt;p&gt;To make sure this worked, let's print out one item from the json data. Work with your table to add a print statement to print the second item in the json_data list.&lt;/p&gt;

&lt;h3 id="installing-nltk"&gt;Installing NLTK&lt;/h3&gt;

&lt;p&gt;We are interested in the languaged used in the "description" fields across all the "cooking" items in the DPLA database. Fortunately, there is good support within Python for text analysis and one power library we can use is the Natural Language ToolKit (or NLTK).&lt;/p&gt;

&lt;p&gt;To install NLTK, let's go back to our terminal and use pip.&lt;/p&gt;

&lt;p&gt;Run &lt;span class="command"&gt;pip install nltk&lt;/span&gt;. You may need to use &lt;span class="command"&gt;sudo pip install nltk&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Now, go back to your file and import nltk at the top of the file.&lt;/p&gt;

&lt;p&gt;There are also a number of datasets available for use with nltk. For our purposes, we will only be using the 'stopwords' dataset, but browse the list of all the datasets you could download and use at &lt;a href="http://www.nltk.org/nltk_data/"&gt;http://www.nltk.org/nltk_data/&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;To download the stopwords, we are going to go back into the Python Interactive Shell. Run &lt;span class="command"&gt;python&lt;/span&gt;. Your terminal window should now look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Python 2.7.5 (default, Mar  9 2014, 22:15:05)
[GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
&amp;gt;&amp;gt;&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Type &lt;span class="command"&gt;import nltk&lt;/span&gt; and press enter.&lt;/p&gt;

&lt;p&gt;Next type &lt;span class="command"&gt;nltk.download('stopwords')&lt;/span&gt; and press enter.&lt;/p&gt;

&lt;p&gt;Great!&lt;/p&gt;

&lt;p&gt;Now we need to load these stopwords into our Python file. Switching back to our "my_second_script.py" file, add &lt;span class="command"&gt;from nltk.corpus import stopwords&lt;/span&gt; to the top of the file.&lt;/p&gt;

&lt;p&gt;We are now ready to start working with our data.&lt;/p&gt;

&lt;h3 id="what-we-learned"&gt;What We Learned&lt;/h3&gt;

&lt;p&gt;In this module, we have reviewed and learned:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;to create new files using terminal&lt;/li&gt;
  &lt;li&gt;to load json data from a file &lt;/li&gt;
  &lt;li&gt;to download libraries using pip&lt;/li&gt;
  &lt;li&gt;to download datasets within libraries&lt;/li&gt;
  &lt;li&gt;to load libraries into our scripts&lt;/li&gt;
&lt;/ul&gt;

</content>
  </entry>
  <entry>
    <title>Writings Search Results to a File</title>
    <link rel="alternate" href="http://blog.url.com/modules/module09-write.html"/>
    <id>http://blog.url.com/modules/module09-write.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-12T00:16:01-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;In this module, we will focus on writing the results of our functions to a text file. This gives us a local copy of the data so that we only hit the DPLA servers once for the entire collection of files.&lt;/p&gt;

&lt;h3 id="opening-a-new-file"&gt;Opening a new file&lt;/h3&gt;

&lt;p&gt;Our first step is to create and open a new file that we will save our search results to. In your "my_first_script.py" file, right after you declare the "all_records" variable, add:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f = open("search_results.json", "w")
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here you are combining a function - open("search_results.json") - with the declaration of a variable. "Open" also creates a file if the file does not already exist on your computer. The "w" indicates that the file should be opened as "write". One thing to note about "w" - "write" gives the computer permission to overwrite the data inside the file, which is why we are opening the file once and writing the whole array at the end. If we were to write each item as we looped, we would end up with only the last item in the file. It would write and overwrite each item as it went along. If you need to write inside a loop, you can open the file as "a". This tells the computer to "append" the information to the end of the file, rather than overwrite the existing information. &lt;/p&gt;

&lt;h3 id="creating-a-write-function"&gt;Creating a "Write" Function&lt;/h3&gt;

&lt;p&gt;Now that we have a file to write to, let's create a third function to write our search results to the file. You can put this after &lt;span class="command"&gt;f = open("search_results.json", "w")&lt;/span&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def save_results():
	data = json.dumps(all_records)
	f.write(data)
	f.close
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, we are creating a variable called "data". Working with json objects in python requires the use of another library, called "json", and json.dumps is an operation that helps python know how to interact with all the data in "all_records". Now, because we are using a new library, we also need to import that library at the beginning of the file. Add &lt;span class="command"&gt;import json&lt;/span&gt; to the top of the file. Next, we are taking our file variable from the first part of the module and writing all of the information contained in "data" to it. We then close the file.&lt;/p&gt;

&lt;h3 id="calling-the-write-function"&gt;Calling the "Write" Function&lt;/h3&gt;

&lt;p&gt;To run the "save_results" function, we can now call the function at the end of our file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;save_results()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our file should now look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from dpla.api import DPLA
import json

dpla = DPLA('Your-Key-Here')

# result = dpla.search('cooking')
# print result.items[1]
	
all_records = []
f = open('search_results.json', 'w')

def save_results():
	data = json.dumps(all_records)
	f.write(data)
	f.close

def pull_records(pages, end, size):
	while(pages &amp;lt;= end):
		paged_search = dpla.search(q='cooking', page_size=size, page=pages)
		save_each(paged_search)
		print "finished page " + str(pages)
		pages = pages + 1

def save_each(n):
	for each in n.items:
		all_records.append(each)

pull_records(2, 5, 50)

print all_records[150]

save_results()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Test that everything is working by running your "my_first_script.py" in terminal.
You can open the "search_results.json" file to check that 200 items made it in.&lt;/p&gt;

&lt;h3 id="saving-all-the-search-results"&gt;Saving all the Search Results&lt;/h3&gt;

&lt;p&gt;Now, let's change the parameters we pass to the "pull_records" function to get all of the search results.&lt;/p&gt;

&lt;p&gt;Remember, the number we pass to "pull_records" is the first page, the second number is the last page, and the third number is the number of items per page. The DPLA will cap us at 500 items per page, so let's take '500' for our third variable. We also want to start with the first page, so '1' is our first variable.&lt;/p&gt;

&lt;p&gt;To figure out the value we want for "end", we need to do a little math. If we have 10,909 items and can get 500 items a page, how many pages do we have to work through?&lt;/p&gt;

&lt;p&gt;Update your "pull_records" line to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pull_records(1, 22, 500)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save and run your script. This will take a bit of time, so get up, stretch, and get some coffee!&lt;/p&gt;

&lt;p&gt;Bonus Challege: Can you write a function that determines the total number of pages from the search results?&lt;/p&gt;

&lt;h3 id="what-we-learned"&gt;What We Learned&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;To create and open a new file&lt;/li&gt;
  &lt;li&gt;The difference between "write" and "append" as ways of opening a file&lt;/li&gt;
  &lt;li&gt;To write json to a file&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry>
    <title>Looping through the Pages</title>
    <link rel="alternate" href="http://blog.url.com/modules/module08-whileloop.html"/>
    <id>http://blog.url.com/modules/module08-whileloop.html</id>
    <published>2014-09-18T20:00:00-04:00</published>
    <updated>2014-10-11T16:54:00-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;In this module, we will add another loop to our "pull records" function that allows us to move through more than one page of search results.&lt;/p&gt;

&lt;h3 id="introducing-the-while-loop"&gt;Introducing the While Loop&lt;/h3&gt;

&lt;p&gt;The "for loop" allows us to do something to each item in a list. The "while loop" is a powerful tool that tells the computer to continue doing something as long as some criteria is true. We can use the while loop and a "counter" to work through all of the pages of search results.&lt;/p&gt;

&lt;p&gt;To use a while loop, let's look again at our "pull records" function.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def pull_records(pages, end, size):
	paged_search = dpla.search(q='cooking', page_size=size, page=pages)
	save_each(paged_search)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remember, "pages" stands for the first page and "end" stands for the last page of search results we want. We want this function to run for every page of search results. In other words, if the page number is less than or equal to the total number of pages available, we want to get the search results from that page. Once we hit the end, we want to stop.&lt;/p&gt;

&lt;p&gt;To write this logic in code, we will add:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while(pages &amp;lt;= end):
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;so that our function now looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def pull_records(pages, end, size):
	while(pages &amp;lt;= end):
		paged_search = dpla.search(q='cooking', page_size=size, page=pages)
		save_each(paged_search)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id="adding-a-counter"&gt;Adding a Counter&lt;/h3&gt;

&lt;p&gt;Can you see the problem with our current function? As it currently stands, "pages" is always less than "end" because it never increases. This means we would get stuck in an "infinite loop" if we tried to run the code right now.&lt;/p&gt;

&lt;p&gt;To avoid the infinite loop, we need to increase the value of "pages" each time we work through the loop. We can do this by overwriting the value of "pages" to be "pages + 1".&lt;/p&gt;

&lt;p&gt;After &lt;span class="command"&gt;save_each(paged_search)&lt;/span&gt; add:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pages = pages + 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let's also add a print command to check that things are working as we expect. Above &lt;span class="command"&gt;pages = pages + 1&lt;/span&gt; add:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print "finished page " + str(pages)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our file should now look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from dpla.api import DPLA

dpla = DPLA('Your-Key-Here')

# result = dpla.search('cooking')
# print result.items[1]

all_records = []

def pull_records(pages, end, size):
	while(pages &amp;lt;= end):
		paged_search = dpla.search(q='cooking', page_size=size, page=pages)
		save_each(paged_search)
		print "finished page " + str(pages)
		pages = pages + 1

def save_each(n):
	for each in n.items:
		all_records.append(each)

pull_records(2, 3, 50)

print all_records[40]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let's test our function on a subset of the pages. Change &lt;span class="command"&gt;pull_records(2, 3, 50)&lt;/span&gt; to &lt;span class="command"&gt;pull_records(2, 5, 50)&lt;/span&gt; and change &lt;span class="command"&gt;print all_records[40]&lt;/span&gt; to &lt;span class="command"&gt;print all_records[150]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Save and run in Terminal.&lt;/p&gt;

&lt;h3 id="what-we-learned"&gt;What We Learned:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;To use a while loop and counter&lt;/li&gt;
  &lt;li&gt;To test on a subset of the data&lt;/li&gt;
  &lt;li&gt;To use "print" to check our functions along the way&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
</feed>
