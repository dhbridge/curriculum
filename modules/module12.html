<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="" />
	  <meta name="viewport" content="width=device-width, user-scalable=no">
	
    <link href='http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic|Nova+Square' rel='stylesheet' type='text/css'>
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

    <title>Analyzing a Subset of the Data (Part 2)</title>

    <link href="../stylesheets/codes.css" rel="stylesheet" type="text/css" />
    <link href="../stylesheets/style.css" rel="stylesheet" type="text/css" />
  </head>

  <body class="modules modules_module12">
    <!-- HEADER -->
        <header>
				<div class="wrap">
	    <h1 id="project_title">DH Bridge</h1>
	    <h2 class="tag">Encouraging Computational Thinking and Digital Skills in the Humanities</h2>

	    <nav id="pages">
	    	<a href="/">Computational Thinking</a>
	    	<a href="http://dhbridge.org">About DH Bridge</a>
	    </nav>
	</div>
        </header> 
        <!-- MAIN CONTENT -->
        <div class="wrap">
            <div id="main">
              <article>
                		<section id="sidebar">
			<h2>Modules</h2>
			<a href="/modules/installation.html">Installation Instructions</a>

			<ol>
			<li><a href="/modules/module01.html">Working with your Computer</a></li>
			<li><a href="/modules/module02.html">Thinking about Data</a></li>
			<li><a href="/modules/module03.html">Getting Data</a></li>
			<li><a href="/modules/module04.html">Working with the API</a></li>
			<li><a href="/modules/module05.html">Writing Script Files</a></li>
			<li><a href="/modules/module06.html">Phase Two</a></li>
			<li><a href="/modules/module07.html">Using Functions and Loops</a></li>
			<li><a href="/modules/module08.html">Using While-Loops</a></li>
			<li><a href="/modules/module09.html">Writing Results to File</a></li>
			<li><a href="/modules/module10.html">Working with Local Data</a></li>
			<li><a href="/modules/module11.html">Analyzing the Data (Part 1)</a></li>
			<li><a href="/modules/module12.html">Analyzing the Data (Part 2)</a></li>
			<li><a href="/modules/module13.html">Saving Results to a File</a></li>
			</ol>
		</section>

                <section id="content">
                        <h2>
          Analyzing a Subset of the Data (Part 2)
      </h2>
       <p>In this module we will learn to:</p>

<ol>
  <li>normalize our text</li>
  <li>use the Frequency Distribution library to find pattern in our text </li>
</ol>

<h3 id="normalizing-the-text">Normalizing the text</h3>

<p>While we can get a lot of information from all of the text, we can also find additional patterns once we do what is called 'normalizing' the text. This entails removing all of the punctuation marks, and transforming all the words to lower case. It also involves removing the small, connection words such as 'the' and 'a', which are very common in a text, but carry less semantic meaning than nouns, verbs, and adjectives.</p>

<p>To clean up the text, we need to work through each words and and run it through a series of checks or filters. </p>

<p>Open your 'text_mining.py' file. First, comment out <span class="command">print text.similar('Pot')</span>. Next, let's create a new function that normalizes the text:</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22</pre></td><td class="code"><pre># Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize

# Define Variables
with open('text_results.txt, 'r') as file:
    cooking_text = file.read().decode('utf8')

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economy')
#print text.collocations()
#print text.similar('Pot')
</pre></td></tr></tbody></table>
</div>

<p>To work through the words, we can use a new 'for loop' and save all of the approved words into a new array. Create your new array at the end of the 'Define Variables' section.</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7</pre></td><td class="code"><pre>word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Create list of words
</pre></td></tr></tbody></table>
</div>

<p>First, for each word, let's check if it is alpha-numeric:</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8</pre></td><td class="code"><pre>word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation
        if word.isalpha()
</pre></td></tr></tbody></table>
</div>

<p>Next, let's check the word against the NLTK collection of stopwords. First, let's load in the NLTK stopwords and assign them to the variable 'stopwords':</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16</pre></td><td class="code"><pre># Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize

# Define Variables
with open('text_results.txt, 'r') as file:
    cooking_text = file.read().decode('utf8')

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Load in Stopwords Library
stopwords = stopwords.words('english')

word_set = []
</pre></td></tr></tbody></table>
</div>

<p>Next, we need to transform each word to lower-case, because the stopwords list won't catch the uppercase words. We do this by adding <span class="command">.lower()</span> to word as we check the words against the 'stopwords' collection. </p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9</pre></td><td class="code"><pre>word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
</pre></td></tr></tbody></table>
</div>

<p>Finally, we will add those words that pass through the filters to the 'word_set' list and tell the function to return to us the whole list once it is finished.</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11</pre></td><td class="code"><pre>word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
            word_set.append(word.lower())
    return word_set
</pre></td></tr></tbody></table>
</div>

<p>The last step is to call our 'normalize_text' function and pass in our text.</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18</pre></td><td class="code"><pre># Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
            word_set.append(word.lower())
    return word_set

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economy')
#print text.collocations()
#print text.similar('Pot')

normalize_text(text)
</pre></td></tr></tbody></table>
</div>

<p>Well done! We're now ready to calculate word frequencies using the Frequency Distribution library.</p>

<h3 id="get-word-frequencies">Get Word Frequencies</h3>

<p>Our python file should now look like this: </p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35</pre></td><td class="code"><pre># Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize

# Define Variables
with open('text_results.txt, 'r') as file:
    cooking_text = file.read().decode('utf8')

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Load in Stopwords Library
stopwords = stopwords.words('english')

word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
            word_set.append(word.lower())
    return word_set

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economy')
#print text.collocations()
#print text.similar('Pot')

normalize_text(text)
</pre></td></tr></tbody></table>
</div>

<p>We need one more library to get word frequencies from our array of approved words. After <span class="command">from nltk import word_tokenize</span>, add:</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1</pre></td><td class="code"><pre>from nltk.probability import *
</pre></td></tr></tbody></table>
</div>

<p>Now, we will use the Frequency Distribution library to get word counts. This may seem a bit like magic, and that is ok. NLTK is a very powerful library, and by using it, we can draw on the work others to build and verify that the functions do what they say they do. There is no need to reinvent the wheel with every new script.</p>

<p>The first step is to run the frequency distribution function and save the results to a variable to make them easier to use. After <span>normalize_text(text)</span> add:</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1</pre></td><td class="code"><pre>fd = FreqDist(word_set)
</pre></td></tr></tbody></table>
</div>

<p>Frequency Distribution goes through all of the words in our array and creates a dictionary that holds the word, and the number of times it occurs in our document. </p>

<p>To see the 50 most common words, add to the bottom of the 'Make Function Calls' section:</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1</pre></td><td class="code"><pre>print fd.most_common(50)
</pre></td></tr></tbody></table>
</div>

<p>Your file should look like:</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39</pre></td><td class="code"><pre># Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.probability import *

# Define Variables
with open('text_results.txt, 'r') as file:
    cooking_text = file.read().decode('utf8')

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Load in Stopwords Library
stopwords = stopwords.words('english')

word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
            word_set.append(word.lower())
    return word_set

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economy')
#print text.collocations()
#print text.similar('Pot')

normalize_text(text)

fd = FreqDist(word_set) 
print fd.most_common(50)
</pre></td></tr></tbody></table>
</div>

<p>Save your script and run. You should start to see some magic happening in the termainal.</p>

<p>You can also see the words that only occur once by adding to the bottom of the 'Make Function Calls' section:</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1</pre></td><td class="code"><pre>print fd.hapaxes()
</pre></td></tr></tbody></table>
</div>

<p>and running the 'text_mining.py' script.</p>

<p>And, for some super magic, you can also generate a plot with:</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1</pre></td><td class="code"><pre>fd.plot(50,cumulative=False)
</pre></td></tr></tbody></table>
</div>

<p>You can comment out print statements along the way if you are seeing too much information. Save your script and run it in the terminal.</p>

<p><span class="left"><a href="module11.html">Previous Module</a></span>
<span class="right"><a href="module13.html">Next Module</a></span></p>



                </section>
              </article>
  			   </div>
  		</div>
  		
  		<footer>
  				<div class="wrap">
		<div id="rights">
		    <p>&copy; CC-BY 2014</p>
		    <p>The DH Bridge curriculum is supported by a microgrant from the <a href="http://ach.org/"><img src="http://dhbridge.org/images/ach-logo.png" width=50px ></a></p>
		</div>

	    <div id="contact">
	        <a href="mailto:bridgingdh@gmail.com"><i class="fa fa-paper-plane"></i></a>
	        <a href="https://twitter.com/dhbridge"><i class="fa fa-twitter"></i></a>
	        <a href="https://github.com/dhbridge/curriculum"><i class="fa fa-github"></i></a>
	    </div>
	</div>
  		</footer>

  </body>
</html>