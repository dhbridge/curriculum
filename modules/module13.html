<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="" />
	  <meta name="viewport" content="width=device-width, user-scalable=no">
	
    <link href='http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic|Nova+Square' rel='stylesheet' type='text/css'>
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

    <title>Save Results to a File</title>

    <link href="../stylesheets/codes.css" rel="stylesheet" type="text/css" />
    <link href="../stylesheets/style.css" rel="stylesheet" type="text/css" />
  </head>

  <body class="modules modules_module13">
    <!-- HEADER -->
        <header>
				<div class="wrap">
	    <h1 id="project_title">DH Bridge</h1>
	    <h2 class="tag">Encouraging Computational Thinking and Digital Skills in the Humanities</h2>

	    <nav id="pages">
	    	<a href="/">Computational Thinking</a>
	    	<a href="http://dhbridge.org">About DH Bridge</a>
	    </nav>
	</div>
        </header> 
        <!-- MAIN CONTENT -->
        <div class="wrap">
            <div id="main">
              <article>
                		<section id="sidebar">
			<h2>Modules</h2>
			<p><a href="/modules/installation.html">Installation Instructions</a></p>
			<p><a href="/modules/coaches-guide.html">Coaches Guide</a></p>

			<ol>
			<li><a href="/modules/module01.html">Working with your Computer</a></li>
			<li><a href="/modules/module02.html">Thinking about Data</a></li>
			<li><a href="/modules/module03.html">Getting Data</a></li>
			<li><a href="/modules/module04.html">Working with the API</a></li>
			<li><a href="/modules/module05.html">Writing Script Files</a></li>
			<li><a href="/modules/module06.html">Phase Two</a></li>
			<li><a href="/modules/module07.html">Using Functions and Loops</a></li>
			<li><a href="/modules/module08.html">Using While-Loops</a></li>
			<li><a href="/modules/module09.html">Writing Results to File</a></li>
			<li><a href="/modules/module10.html">Working with Local Data</a></li>
			<li><a href="/modules/module11.html">Analyzing the Data (Part 1)</a></li>
			<li><a href="/modules/module12.html">Analyzing the Data (Part 2)</a></li>
			<li><a href="/modules/module13.html">Saving Results to a File</a></li>
			<li><a href="/modules/module14.html">Next Steps</a></li>
			</ol>
		</section>

                <section id="content">
                        <h2>
          Save Results to a File
      </h2>
       <p>We are almost there! We have been creating some interesting data on the word frequencies within the description fields. But so far, all of our results are stuck in Terminal, which makes it difficult for us to reuse them. So for this final module, we will write out the results of our count to a CSV (comma separated value) file.</p>

<h3 id="create-a-new-csv-file">Create a New CSV File</h3>

<p>As we did when we wrote our JSON results, we will start by telling Python to open a CSV file and assign to a variable.</p>

<p>Currently our <code>text_mining.py</code> file should look like this:</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40</pre></td><td class="code"><pre># Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.probability import *

# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Load in Stopwords Library
stopwords = stopwords.words('english')

word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
            word_set.append(word.lower())
    return word_set

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economics')
#print text.collocations()
#print text.similar('Pot')

normalize_text(text)

fd = FreqDist(word_set)
print fd.most_common(200)
print fd.hapaxes()
</pre></td></tr></tbody></table>
</div>

<p>To create CSV files, we need to import the <code>csv</code> library, which is preinstalled, but not preloaded in Python. To do that, add <span class="command">import csv</span> to our list of libraries at the top of the file.</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6</pre></td><td class="code"><pre># Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.probability import *
import csv
</pre></td></tr></tbody></table>
</div>

<p>Now, we can create our CSV file right after the line where we opened the JSON file. CSV files open a little differently than text files, in that we open the file with a "writer" helper.</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5</pre></td><td class="code"><pre># Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

file = csv.writer(open('word_frequencies.csv', 'wb'))
</pre></td></tr></tbody></table>
</div>

<p>Now, at the bottom of <code>text_mining.py</code>, we can save the key (the word) and the count (the frequency) as two columns in the CSV file <code>word_frequencies.csv</code>. If you did the graphing challenge, be sure to comment out <span class="command">fd.plot(50,cumulative=False)</span> as well. The plotting function and the write csv functions don't work well together.</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2</pre></td><td class="code"><pre>for key, count in fd.most_common(200):
    file.writerow([key, count])
</pre></td></tr></tbody></table>
</div>

<p>The final product should look like this:</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48</pre></td><td class="code"><pre># Import Libraries
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.probability import *
import csv

# Set Variables
with open('text_results.txt', 'r') as file:
    cooking_text = file.read().decode('utf8')

file = csv.writer(open('word_frequencies.csv', 'w'))

cooking_tokens = word_tokenize(cooking_text)
text = nltk.Text(cooking_tokens)

# Load in Stopwords Library
stopwords = stopwords.words('english')

word_set = []

# Define Functions
def normalize_text(text):
    # Work through all the words in text and filter
    for word in text:
        # Check if word is a word, and not punctuation, AND check against stop words
        if word.isalpha() and word.lower() not in stopwords:
            # If it passes the filters, save to word_set
            word_set.append(word.lower())
    return word_set

# Make Function Calls
#print cooking_text[0:20]
#print cooking_tokens[0:10]
#print text.concordance('economics')
#print text.collocations()
#print text.similar('Pot')

normalize_text(text)

fd = FreqDist(word_set)
print fd.most_common(200)
#print fd.hapaxes()
#fd.plot(50,cumulative=False)

# Print results to a CSV file
for key, count in fd.most_common(200):
    file.writerow([key, count])
</pre></td></tr></tbody></table>
</div>

<p>You can open your CSV file using Terminal by typing or by looking within your "dhb_awesome" directory:</p>

<div class="highlight plaintext"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1</pre></td><td class="code"><pre>open word_frequencies.csv
</pre></td></tr></tbody></table>
</div>

<p>The file will most likely open in Excel or a similar program. </p>

<p>Look over your results. What patterns strike you as interesting? As expected? As unexpected? What additional questions do these word frequencies raise? Now that you have this data, what additional information do you need to know to interpret the patterns we see here?</p>

<p><span class="left"><a href="module12.html">Previous Module</a></span>
<span class="right"><a href="module14.html">Next Module</a></span></p>



                </section>
              </article>
  			   </div>
  		</div>
  		
  		<footer>
  				<div class="wrap">
		<div id="rights">
		    <p>&copy; CC-BY 2014</p>
		    <p>The DH Bridge curriculum is supported by a microgrant from the <a href="http://ach.org/"><img src="http://dhbridge.org/images/ach-logo.png" width=50px ></a></p>
		</div>

	    <div id="contact">
	        <a href="mailto:bridgingdh@gmail.com"><i class="fa fa-paper-plane"></i></a>
	        <a href="https://twitter.com/dhbridge"><i class="fa fa-twitter"></i></a>
	        <a href="https://github.com/dhbridge/curriculum"><i class="fa fa-github"></i></a>
	    </div>
	</div>
  		</footer>

  </body>
</html>