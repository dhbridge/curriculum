---
title: Analyzing the Data (Part 1)
date: 2014-09-19
---

In this module we will:

1. install NLTK library
2. use NLTK to do see patterns in the text

### Installing NLTK

We are interested in the languaged used in the "description" fields across all the "cooking" items in the DPLA database. Fortunately, there is good support within Python for text analysis and one power library we can use is the Natural Language ToolKit (or NLTK).

To install NLTK, let's go back to our terminal and use pip.

Run <span class = "command">pip install nltk</span>. You may need to use <span class="command">sudo pip install nltk</span>.

Now, go back to your file and import nltk at the top of the file.

There are also a number of datasets available for use with nltk. For our purposes, we will only be using the 'stopwords' dataset, but you can browse the list of all the datasets you could download and use at [http://www.nltk.org/nltk_data/](http://www.nltk.org/nltk_data/). 

To download the stopwords, we are going to go back into the Python Interactive Shell. Run <span class="command">python</span>. Your terminal window should now look something like this:

    Python 2.7.5 (default, Mar  9 2014, 22:15:05)
    [GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)] on darwin
    Type "help", "copyright", "credits" or "license" for more information.
    >>> 

Type <span class="command">import nltk</span> and press enter.

Next type <span class="command">nltk.download('stopwords')</span> and press enter.

Once you see 
    
    True
    >>>

you have successfully downloaded the stopwords file. You can now exit the Python Interactive Shell using <span class="command">quit()</span>

### Let's start text mining

Let's create a third script file, 'text_mining.py':

    touch text_mining.py

(or for Windows):
    
    New-Item -ItemType file text_mining.py

And open that script file in your text editor:

    open text_mining.py

(and for Windows):
    
    Start notepad++ text_mining.py

Let's start by importing the nltk library and the stopwords:

    import nltk
    from nltk.corpus import stopwords

Next let's load in our text file:

    with open('text_results.txt') as file:
        cooking_text = file.read()

We can display part of our text to make sure everything is loading correctly:

    import nltk
    from nltk.corpus import stopwords

    with open('text_results.txt') as file:
        cooking_text = file.read()

    print cooking_text[0:20]

Save a run your new script in terminal. One thing to note here is that Python treats the text as a list of letters. 

But we want to work with the words, so let's use a function called 'tokenize' from the NLTK library. 'Tokenize' breaks our text into a list of words.

After 'from nltk.corpus import stopwords', add:
    from nltk import word_tokenize

Now let's comment out our print statement and transform our sentences into tokens. To see what has happened, let's also print the first 10 tokens.

    import nltk
    from nltk.corpus import stopwords
    from nltk import word_tokenize

    with open('text_results.txt') as file:
        cooking_text = file.read()

    #print cooking_text[0:20]

    cooking_tokens = word_tokenize(cooking_text)
    print cooking_tokens[0:10]

Next, we need to do one more transformation on our words so that they will play nicely with NLTK. Comment out 'print cooking_tokens[0:10]' and beneath it add:

    text = nltk.Text(cooking_tokens)

Your file should now look like this:

    import nltk
    from nltk.corpus import stopwords
    from nltk import word_tokenize

    with open('text_results.txt') as file:
        cooking_text = file.read()

    # print cooking_text[0:20]

    cooking_tokens = word_tokenize(cooking_text)
    # print cooking_tokens[0:10]

    text = nltk.Text(cooking_tokens)

The first thing we can do to get a sense of the words in our dataset is to use the 'concordance' function within NLTK. This will print all the instances of a word with the surrounding words for context.

After <span class="command">text = nltk.Text(cooking_tokens)</span>, add:

    print text.concordance('cooking')

Save and run your script.

Pretty cool! Now try changing 'cooking' to 'economics':

    print text.concordance('cooking')

Try some other words to get a sense of our search results document.

Another useful command is 'collocation'. This shows us all the words that tend to appear together throughout the corpus.

Comment out <span class="command">print text.concordance('your_last_word')</span> and add 

    print text.collocations()

Save and run your script.

There are probably quite a few dates and names showing up. 

One more function that is useful to surveying our data is 'similar'. This shows us words that are used similarly to the word we give it.

Comment out <span class="command"> print text.collocations()</span> and add:

    print text.similar('Pot')

Your script should now look like:

    import nltk
    from nltk.corpus import stopwords
    from nltk import word_tokenize

    with open('text_results.txt') as file:
        cooking_text = file.read()
    # print cooking_text[0:20]

    cooking_tokens = word_tokenize(cooking_text)
    # print cooking_tokens[0:10]

    text = nltk.Text(cooking_tokens)

    #print text[:20]
    #print text.concordance('economy')
    #print text.collocations()

    print text.similar('Pot')

What other patterns might be interesting to know about the words used to describe objects related to 'cooking'?

In the next module, we will look at word counts to find the most common words used across all of the different DPLA contributors. 

<span class="left">[Previous Module](module10.html)</span>
<span class="right">[Next Module](module12.html)</span>