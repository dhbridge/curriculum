---
title: Analyzing the Data (Part 1)
date: 2014-09-19
---

Up to this point, we have been focusing on molding the data we got back from the DPLA into different formats. First, we chose the part of the data we wanted to save locally and looped through all of the search result "pages" to get our large dataset. Then, because our research question involved language use and text, we transformed the data again into a format that suits the kind of analysis we want to do. Now, we get to take the results of our hard work and start to interrogate our data.

In this module we will:

1. install NLTK library
2. use NLTK to do see patterns in the text

### 1. Installing NLTK

We are interested in the language used in the three fields we singled out across all the "cooking" items in the DPLA database. Fortunately, there is good support within Python for text analysis and one powerful library we can use is the Natural Language ToolKit (or NLTK).

To install NLTK, let's go back to our Terminal and use `pip`.

Run <span class = "command">pip install nltk</span>. You may need to use <span class="command">sudo pip install nltk</span>  (Mac).

There are also a number of datasets available for use with NLTK. For our purposes, we will only be using the "stopwords" dataset. You can browse the list of all the datasets you could download and use at [http://www.nltk.org/nltk_data/](http://www.nltk.org/nltk_data/). 

To download the stopwords, we are going back into the Python Interactive Shell. Run <span class="command">python</span>. Your Terminal window should now look something like this:

    Python 2.7.5 (default, Mar  9 2014, 22:15:05)
    [GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)] on darwin
    Type "help", "copyright", "credits" or "license" for more information.
    >>> 

Type <span class="command">import nltk</span> and press Enter.

Next type <span class="command">nltk.download('stopwords')</span> and press Enter.

Once you see 
    
    True
    >>>

you have successfully downloaded the stopwords file. 

You will also need to download a tokenizing library. Still in the Python shell, run

    nltk.download('punkt')

Again, once you see

    True
    >>>

the download is complete.

You can now exit the Python Interactive Shell using <span class="command">quit()</span>

### Let's Start Text Mining

Let's create a third script file, "text_mining.py":

    touch text_mining.py

(Windows):
    
    New-Item -ItemType file text_mining.py

And open that script file in your text editor:

    open text_mining.py

(Windows):
    
    Start notepad++ text_mining.py

Let's start by importing the NLTK library and the stopwords:
    
    # Import Libraries
    import nltk
    from nltk.corpus import stopwords

Next let's load in our text file:

    # Set Variables
    with open('text_results.txt', 'r') as file:
        cooking_text = file.read().decode('utf8')

Let's add a "print" command so we can display part of our text to make sure everything is loading correctly so far:

    # Import Libraries
    import nltk
    from nltk.corpus import stopwords

    # Set Variables
    with open('text_results.txt', 'r') as file:
        cooking_text = file.read().decode('utf8')

    # Define Functions

    # Make Function Calls
    print cooking_text[0:20]

Save and run this script in Terminal. Let's take a look at the results. One thing to note here is that Python treats the text as a list of letters. 

For our purposes, we want to work with the words, so let's use a function called "tokenize" from the NLTK library. What "tokenize" does is go through our text and, using white space and punctuation marks, bundles the letters together into "words". When we started, the computer considered our text to be a string of letters; after tokenizing, the computer sees the text as a list of words. 

After "from nltk.corpus import stopwords" in the variables section, add:

    from nltk import word_tokenize

Now let's comment out the last print statement and transform our sentences into tokens using the "word_tokenize" function. To see what has happened, let's also print the first 10 tokens.

    # Import Libraries
    import nltk
    from nltk.corpus import stopwords
    from nltk import word_tokenize

    # Set Variables
    with open('text_results.txt', 'r') as file:
        cooking_text = file.read().decode('utf8')

    cooking_tokens = word_tokenize(cooking_text)

    # Define Functions

    # Make Function Calls
    #print cooking_text[0:20]
    print cooking_tokens[0:10]

Next, we need to do one more transformation on our words so that they will play nicely with NLTK. Comment out "print cooking_tokens[0:10]" and add to the variables section after <span class="command">cooking_tokens...</span>:

    text = nltk.Text(cooking_tokens)

This is a NLTK function that takes the tokens and allows us to move back and forth between a "text view" and a "list view" of our document.

Your file should now look like this:

    # Import Libraries
    import nltk
    from nltk.corpus import stopwords
    from nltk import word_tokenize

    # Set Variables
    with open('text_results.txt', 'r') as file:
        cooking_text = file.read().decode('utf8')

    cooking_tokens = word_tokenize(cooking_text)
    text = nltk.Text(cooking_tokens)

    # Define Functions

    # Make Function Calls
    #print cooking_text[0:20]
    #print cooking_tokens[0:10]
    

The first thing we can do to get a sense of the words in our dataset is to use the "concordance" function within NLTK. This will print all the instances of a word with the surrounding words for context.

After <span class="command">#print cooking_tokens[0:10]</span>, add:

    print text.concordance('cooking')

Save and run your script.

Pretty cool! Now change "cooking" to "economics", save and run the script, and see what the output is:

    print text.concordance('economics')

Try some other words to get a sense of the word usage in the "text_results" file. You can either replace the word or add additional print statements.

Another useful command is "collocation". This shows us all the words that tend to appear together throughout the corpus.

Comment out <span class="command">print text.concordance('your_last_word')</span> and add 

    print text.collocations()

Save and run your script.

One more function that is useful for surveying our data is "similar". This shows us words that are used similarly to the word we give it.

Comment out <span class="command"> print text.collocations()</span> and add:

    print text.similar('Pot')

Your script should now look like:

    # Import Libraries
    import nltk
    from nltk.corpus import stopwords
    from nltk import word_tokenize

    # Set Variables
    with open('text_results.txt', 'r') as file:
        cooking_text = file.read().decode('utf8')

    cooking_tokens = word_tokenize(cooking_text)
    text = nltk.Text(cooking_tokens)

    # Define Functions

    # Make Function Calls
    #print cooking_text[0:20]
    #print cooking_tokens[0:10]
    #print text.concordance('economics')
    #print text.collocations()

    print text.similar('Pot')

What other patterns might be interesting to know about the words used to describe objects related to "cooking"?

In the next module, we will look at word counts to find the most common words used across all of the different DPLA contributors. 

<span class="left">[Previous Module](module10.html)</span>
<span class="right">[Next Module](module12.html)</span>
